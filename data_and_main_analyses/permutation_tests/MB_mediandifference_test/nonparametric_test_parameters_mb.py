from google.cloud import storage
import os
from io import StringIO # if going with no saving csv file
import random
# say where your private key to google cloud exists
print('HERE')
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'mf_key.json'
print('WORKED!!')


#parameters empirical fits
# betas_pred=[0.5397526340591204, 2.6988346343198955, 0.359671631579266, 1.2137781107575403, 1.0044983417056104, 1.0682510117675583, 1.3004626029990443, 3.7024619793460003, 2.6910059524432466, 5.300748937123223, 1.2468136367323945, 1.5788646719809953, 2.57753633813574, 1.335936903807894, 2.934231724992694, 0.6280011048513836, 4.72959727983321, 6.116878145649378, 0.12209002290192657, 3.357468471494464, 0.2815289701410741, 0.9792432448004045, 1.0159340483770112, 1.030997065691311, 3.3994669935348507, 3.7575939051349776, 0.8515928128204351, 1.2160447156186334, 5.512373929604602, 2.642321661577515, 0.3769831832578884, 0.4249932882575361, 1.5531566493859732, 1.1512506617094116, 6.502212812316734, 0.6529773512969071, 0.8337129236568054, 1.695792828719969, 0.9648779345982504, 3.053801264516185, 0.4123761881634834, 0.6440768122613739, 0.42786837064749195, 0.7165757693590467, 3.555319241181647, 0.5020726283255647, 0.7880362529225212, 0.3677140025853202, 1.2832450929214607, 6.8644479276874355, 1.2693639424100165, 1.398500535727746, 1.5311352125100455, 0.34309394768792145, 1.105823306735209, 1.9494759130299728, 0.9215871650955325, 0.6826310034979857, 1.4805663763726644, 1.2741142227293696, 0.7639297340725586, 0.7185425892827403, 0.542987155053475, 0.8216197113806273, 1.1994147564255158, 2.0365339387825028, 0.27647622034834607, 1.0057669892938077, 0.9404810129133665, 0.17054465602179247, 2.414689209598187, 1.222232359032166, 0.6531601036019596, 0.6358905244267098, 2.101743679347694, 0.9714092616423546, 2.282835719794382, 1.8323996364535977, 1.3607818032541434, 0.8024800558988301, 2.5313491139940996, 0.3897549960317875, 5.04325700097629, 1.7106390699706802, 1.3889121444527472, 6.910103964610682, 1.3032516645638375, 1.1171049897493865, 2.0915513520866664, 0.4379205675087667, 3.0809851827381487, 4.511452832595503, 1.1820332881847107, 7.335797709272178, 0.5671059138038548, 1.964467781315347, 0.971561234580851, 9.28618119730119, 0.8399970749869876, 1.5263269393542116, 0.3719798664820576, 1.5364141310178103, 1.3509198571072738, 0.30347480196190363, 1.3704098647275087, 1.983330180797831, 2.94767594450717, 0.5083330399197133, 1.5968765934657125, 2.1421722436408492, 2.566166667904644, 2.244907845599322, 0.8739514886244669, 1.4194864628428139, 0.2625119031940735, 2.879199136391841, 1.2167773964322985, 1.1151389903873439, 1.041718402821921, 2.272770860316566, 0.413540150190243, 1.9639627603231153, 2.293844034598502, 0.9181711503211624, 1.519928758095283, 0.8759600073205441, 2.0192504645355673, 0.8612049196536034, 0.6928055608322781, 1.2644499266762903, 0.18691243660659101, 3.3585183524137667, 1.6405664503042456, 4.661565734944032, 12.940097788158333, 0.30644898348908095, 9.022343276377777, 1.0039477090572593, 0.6014789018699468, 12.395385256537919, 1.246214991044591, 13.046772142547727, 0.9586586213988342, 2.4975041019827335, 0.5866806064159684, 0.8631031851561141, 1.4463840782210295, 0.28065088092214774, 0.9491988774612571, 1.3223489968742863, 0.4450927124957338, 0.3617912993402653, 1.1338531355627894, 0.9514759618325267, 0.47969754964936734, 0.09792212267562571, 0.2800810497843316, 1.4089111240980268, 1.1313973045251446, 1.096464803894938, 7.823064373794357, 0.21765540591436053, 1.779979610158357, 0.30193559989963464, 0.4123317353226951, 3.325281911245376, 3.011967358958072, 2.1487005986759895, 0.7598614297947286, 0.8967733075439333, 1.6354548767440344, 6.153335076716694, 1.9998316563770524, 3.351969025684433, 2.617705889132283, 8.97697407754347, 0.8526262195452555, 1.302161805628992, 1.4033652196472828, 0.2931954227373518, 1.7905646925924297, 0.931878362596033, 0.28017875105841894, 1.1224506993705174, 1.1285426279814326, 0.9782935955142764, 0.9423491032922774, 0.5368285298644698, 0.3675497056998624, 0.8372323322873854, 1.4811229851442778, 0.9010975125922247]
# pred_change=[0.40500188997292386, -1.075227946120189, 0.3229065447914452, 0.09060658779045828, -0.3714877211402951, -1.142828522007377, 0.9978535243788045, -0.8944477639259139, -0.14041768338753868, -1.5713259251052545, -1.8690396861364564, 0.7808579831721265, 0.5898151341551803, 0.9119623097377328, 0.8623786492197993, 0.36733958821041285, 1.1287949139864666, -0.49572528527707826, 0.5679682796377657, 0.7417351560203199, 0.3683925960794459, -0.22815112415228783, 0.24652992019416556, -0.37819853097276135, -0.00973282906955408, 0.7828593023145229, -0.6470651912387886, -0.7815698523992226, -0.6407121241717955, 0.2240653333288702, 0.2506896930060147, 0.6723991883599248, -0.5315429614092045, -0.040597763273010054, 0.11777285700035037, -1.3169545515543084, 0.19827299280602237, -0.18031996690796656, 0.0562952979952226, -0.2405205836140118, -0.8997798906656551, -1.19633175369957, -1.0439332155957493, -0.5483348319492474, 1.2728352198970365, -0.736162959389615, -0.022890576081129432, 0.5188231550930232, 0.41378765896176634, 0.8444732019860093, -0.18259800711435398, -0.33041898024668426, 0.4770782242957737, -0.3085568532488411, 0.3654318732074187, -0.1620942748713185, 0.09462599966474185, 0.45946899292074933, -0.34329711433640475, -0.10401026806921308, 0.24813401068544344, -0.06099634693615326, -0.23257184103380993, 0.009001440175360404, -0.13033303513611377, -0.2584986295753259, -0.4455941697580324, 0.11606328670463037, 0.09152514562944256, -0.3961650761569216, 0.5711003063650713, -0.4953504815523775, -0.3244004511106349, -1.1820715910817854, 0.09558356942493332, 0.13290247350902118, 0.3446892100337886, -2.2214737520620496, -0.05654661585442411, -0.016149208957354392, -1.024352287648793, -0.4939591011683409, -1.0336638514661372, -0.616452102332903, -0.5314439589344913, -0.5572647531298398, -0.23474218918335055, -0.5064083405853734, -0.21507314487035245, 1.3280456516940737, -0.2349487271393493, 0.495069960084356, -0.3932251121297581, 0.8777528810519212, -0.021110190207057854, -0.2522483849662258, 0.016383991570848987, -1.2827212854964936, -0.6008398252830283, -0.10366829959961678, 1.5155043790659426, -1.266137414035116, -0.17275092419623228, -0.6158989649666436, -0.21412311340025197, -0.01393744802624651, -1.1702766984108481, -0.7237552286915575, -0.2768101780704514, -0.49074482752574, 0.5888226139408743, -0.26310302716109024, -0.41270057469757027, -0.44305376010330366, -1.0814243342155736, -0.9252196603750079, -0.25936939693244776, -0.942730615233464, -0.02935046010985397, -0.3695216276982352, 1.086558991573211, 0.4709148270058511, 0.6997029420618062, 0.18322391314289782, -0.1801536936105387, -0.013951012820195674, -0.17333923149032993, 1.3179088326515263, 0.07639673259115017, -0.49549537336656424, -0.43820076020613097, -1.5594301183921386, -0.500672422455235, -1.4501400259371204, 0.32690233014104697, 0.44574027153828494, -0.38034055625611274, 0.0945474282292768, 0.40277273744878095, 0.19840688669978462, 0.1520690567208432, 0.37074210348734016, 0.6668259080446982, 0.7724107091131065, -0.11444634667868894, 0.4972148852734104, 0.7256703191185867, 0.9612889800420084, -0.27683878677042284, -0.6842519176125911, -0.5523557583870534, 0.171449098786793, -0.7078821591740426, 0.0397195309865228, -0.5039590439449728, -0.4440956013746594, -0.5754354544149057, 0.9222780878837086, 1.2890705525122343, 0.05964020995473002, 0.3618853498696365, 0.8045073256239973, 0.016738206303631157, -1.0109344486774716, -0.4776163714118038, -0.2632832260259463, -0.6177886538497268, -0.7591971005213888, -0.404418304013121, -0.28941804864413934, -0.5868998922664801, 0.12422588089741687, -1.1945665683658586, 0.22014092626923903, -0.05863936754379613, 0.5966598038145648, -1.0079103808223484, -0.09593115904915996, -1.1322499322068755, -0.03249040405422516, -1.1203958394465852, -0.41956994524375246, -0.36408774738907984, 0.023187994930804513, -0.24354499183648057, -0.06782480231163418, -1.0725119557851828, 1.2246696649557336, 0.05668375000804948, -0.3463102293028145, -0.4490022089659735, 0.029165072784050956]
# betas_safe=[4.0466327784429375, 2.0543834263398804, 3.4307998495350613, 3.0979640067760412, 2.8491974811399525, 1.6897588513209352, 1.0553754057880327, 3.9311242976896894, 3.203898836529283, 11.064436785835586, 0.673348854375303, 2.0054738347871446, 2.9751847879481628, 1.7487787539620432, 2.5349821685418568, 0.9157394321863503, 13.169464717367, 3.059699550161463, 2.5279907159332127, 11.270735557359979, 4.5005586360079946, 2.9344682717323445, 2.7512846976575864, 4.927641572104338, 2.0538102009982913, 4.796714699080948, 2.038269828873258, 1.0916196049559344, 3.3685878585061766, 13.10943384884129, 1.0941186068653055, 3.115999601953865, 1.906470037225304, 2.3970499797156366, 6.457142961931604, 2.2662250361197924, 1.9943486979563931, 2.569034526361639, 5.445954915908745, 2.1449803194311987, 1.4795183445381832, 0.21428102636402427, 2.785999416552444, 9.82882285832051, 6.764976661121441, 8.2641241982447, 7.606208205343934, 1.915192958780151, 3.4863340203624547, 13.049314864766648, 5.527414405653812, 4.503673768415018, 1.133483377006627, 0.49935519435253906, 3.328081639234209, 1.787437423163266, 1.0201794286619967, 0.6651345492116468, 2.280045031496824, 3.240398369264467, 2.73413792199009, 3.1847575194785005, 2.5956659010945544, 2.7474876413421336, 2.211858336291389, 10.95673599816319, 2.446238450828171, 2.620035292987792, 1.1441080570878124, 1.9296165547654112, 0.6059238274206468, 2.2782361227597354, 4.047227359264475, 1.6658842595904582, 7.238226482512548, 4.737468804264277, 22.214504614320667, 0.1544267899906126, 1.95377848315236, 2.7589658152354564, 4.111457298861261, 0.9912007656651641, 9.824406810093643, 2.4256984064087046, 0.755362895545104, 7.902076181342627, 0.9276274626520874, 0.7530567067186706, 1.9827505011167537, 4.432658203390463, 5.201218759418316, 2.246989963212515, 1.398582471591643, 32.39622123144565, 1.960942432113539, 2.368489890586139, 2.1745214941200754, 20.03533506075816, 4.450262283487907, 0.9065348544395463, 2.2680287695886787, 2.080500447994334, 2.13749367979306, 2.459696073517992, 2.357141949867484, 1.080972510387528, 0.6381919317512792, 1.0898918773691846, 20.18985242394799, 3.887136657700891, 4.10623703271747, 4.5643359749917956, 17.737440773295024, 0.7467255966638277, 2.3330926895819784, 5.3009071307965145, 0.6749040828488725, 0.610692322114404, 2.3672877369402627, 20.801790731093394, 1.4785349982484695, 2.6308877338401837, 9.219853837391833, 2.766650893473879, 2.3230664276818542, 3.15300761409569, 7.875845626462279, 2.3921748546142307, 1.4197629150750632, 0.8108110634375378, 5.214476821115327, 6.514157711320938, 6.45469420658683, 13.986532059770267, 3.9175886754676497, 1.884451040222778, 8.978697936506965, 1.462302300513553, 1.3130423745600697, 3.100665086967013, 1.6885171133476906, 0.3942083847467482, 1.5746236266053983, 3.7509539231672626, 0.5827608107208136, 1.9372606504607592, 13.91932817806955, 3.6977506936594025, 0.35529118630115053, 3.2788455223368675, 16.144481114564336, 3.5934829223915883, 8.22757703260253, 2.475673441132199, 2.3032023977853817, 0.9151288790606118, 12.535396517351629, 3.2640795592082354, 2.3812394578408327, 1.930679526651677, 22.27356891160339, 3.417244031388053, 1.5497789817810668, 3.1180351134955777, 0.40717766746168627, 1.8999060014053932, 4.344726037802298, 0.3187310475511279, 3.3121957327168006, 5.006920225627224, 4.073734259701337, 3.884494530438761, 3.928481472070067, 1.7825917807932945, 1.217010541861733, 14.316142534479239, 1.967509020672825, 1.2261754683909007, 0.3869888839587902, 13.85739375316393, 5.694571803521891, 1.5419370626084508, 2.3828228895387547, 2.4708892667810316, 1.822704235838809, 3.0380633446737058, 2.618440436581581, 4.053981623526523, 0.8623248863492157, 6.696597084523097, 2.1609569568677296, 3.090373803149407]
# safe_change=[0.796468843012494, 0.41908352002723587, 0.10900324669872011, 0.32627520297050144, 0.7158892527412305, 1.4789454074547972, 0.538387594799611, -0.14914157260606425, 0.5666327416925446, 0.5101586622853146, 1.3703905639538108, 0.3177034280181234, 1.2185570079985488, 1.730268739735927, 0.8838308538142396, -0.7631342367713323, 0.6117888817940732, 0.8793662301298841, 1.378231553162596, 1.0714181063715873, 0.5145092128749746, 0.4653304286328828, 0.3803289926301879, 0.8398846217048653, -0.05334020810909785, 0.383293252403952, -0.020104895290901585, 0.877998151503099, -0.5493387601814026, 0.673804719397777, 0.6462168227523986, 0.8364644258190371, 0.2550932202052845, 0.39779731960318715, 0.2844540343653174, -0.5485388331687165, -0.15717658260753917, 0.09522168094894883, 0.6935363330107068, 0.3144840829538643, -0.3542750319086108, -0.2525225414498369, -1.143792279180535, 0.007599419634438355, 1.6429251313472957, 1.1087402536889752, 0.7682306323583482, 0.7163039792813873, 0.9573659777469402, 1.0733831686151296, 0.38850940281972063, 0.028696940129619136, 1.1373862516232267, -0.19422364439407167, 0.23065432811232636, 0.6588477789905027, -1.1733774823945653, 1.6907839048385185, 0.30687693855017406, 0.4340660401486857, 0.6715386917697589, 0.26162796742139866, 0.0928105515452716, 0.41217991330760567, 0.5230041003793875, 0.26111667189439497, 0.5032935144701821, 0.038444925779188954, -0.7021714239679172, 1.121973744493468, -0.48965496734836733, 0.5633320902106141, 0.4724182667364659, -0.8385145092881512, 0.7744206883798853, 0.7889843326855748, 0.5027638154398476, 0.5318407147828611, 0.6049261766306036, 0.6550989074618261, 1.2286027716858594, 0.07461332552615486, 1.0265754313113322, 0.4824542149541697, 0.1502169904534215, 0.5682246326305134, -0.16575737113387803, 0.6001307973870781, 0.5585947957531355, 1.977999015999452, 1.2683284311617413, 0.7700750878597606, 0.32287695584008014, -0.283136423465412, 0.486431428587813, 0.5565237491076671, 0.8062183980978523, 0.8555343924538836, -0.25914984710830297, 0.4062080627932538, 1.8671985486593725, 1.352245683773795, 0.23931318030582338, 0.6436037698257108, 0.4272310616003895, 1.130817970551903, -1.9509589359497779, 0.2752620924898877, 0.5426431610783701, 0.44310088609192516, 0.7580424153283052, 0.4392056992730562, 0.6271776003019599, 0.4645743552148019, 1.2364457729491771, -0.2712979998070765, 1.1374191142008923, 0.4022567513980213, 0.4467285094928914, 0.41259268038412644, 0.510742598078122, 1.4099190191301791, 1.4472812022156425, 0.6231236043985188, 0.6118789374876956, 0.0704671021087787, 0.7908889940208531, 0.7737688067069047, 0.2887795207273433, -1.1243602556039347, 0.02938609130052355, -0.05795631369472754, 1.0587944774970883, 1.0708155383267681, -0.6739124125136965, 1.2969793937232053, 0.47107784781984885, 0.2669166112326337, 1.0153622724611633, 1.5020410777579825, 0.7781894986552553, -0.3885341564001438, 0.15063386206589038, 0.9057358300579816, 0.053372811279497764, 0.3295924755822139, 0.8077040232887197, 2.160723225558525, 0.22229941638805048, 1.244917754922891, 0.7520690231431173, -0.16431467785246898, 1.1044002699484101, 0.35920973228488184, 0.25481543839475324, 1.7813106704006263, 0.42222649437231713, 2.0500027141763817, 2.5813370484734084, 0.5962453720040263, 0.8688191465314844, 0.5427124211088775, 0.7134102588669499, 0.8058494340787968, 0.6370758347804676, 0.0018933763107883612, 1.2751713239006326, 2.473888177455325, 0.470775268175481, 0.5934260098096928, -0.5303034515200274, 0.25763619646367136, -0.915350216438484, 0.9103709919420158, 0.5943518362104545, -0.18599708324125364, -1.019901526536758, 0.4231890469815024, -0.10059327005147868, 1.462796105975805, 0.35546357454363287, 0.8820532926671345, 0.5715666318658403, 0.1956062935869112, -0.4939240217179425, 0.339975461083516, -0.6513076191851245, 1.0586272425107484, 0.022229726590874563, 0.08278219565388634, 0.1501449297918752, 0.17072231809027721]
# beta_mf=[0.542761088549331, 0.3831265720271094, 1.1010504802392338, 0.3003208821337177, 0.21077883933229583, 0.6991655512545731, 0.24451221080110236, 0.31705761913413294, 1.3952689432560978, 0.18802884488787178, 0.8297584668024923, 0.2558139211023259, 0.28805242712159473, 0.5442328365519806, 1.0302419882807226, 0.4675871884820912, 0.24965140459683674, 0.23634356392530148, 0.47184407039443405, 0.5813582735197341, 0.39097889871753094, 0.5688755265948923, 0.19723059236222754, 0.7336857768148388, 0.7663166659872191, 0.392824166885098, 0.4404354705160733, 0.5429947137534076, 0.4846424481193721, 0.6922219838532414, 0.8613379234995817, 1.0214494047096603, 0.27326785235537643, 0.36362805946215465, 0.6172988992788286, 0.46084594186819167, 0.5773195642906173, 0.2862958530534368, 0.27961967832568546, 1.5059699671190752, 0.3922989151101236, 0.9804594700627831, 0.3363902623745081, 1.1961045588938621, 0.5874437228430494, 0.11387121061373884, 0.15942541832851073, 0.7137814664249246, 0.3282518318116558, 0.30090257272082416, 1.3990217788354855, 0.7442457211118186, 2.1576063235055694, 1.3466041197987773, 0.32502470316887205, 0.22153491366557043, 0.7291124456317436, 0.5321086949228278, 0.15747258467173325, 0.16136899482597442, 0.17197874140662286, 0.4836362180608157, 0.20451177360008782, 1.492754811522868, 0.550723908936726, 0.5097106676807931, 0.6121825193556306, 1.7825020892267514, 0.5717870797900172, 0.8352237555353906, 0.7498756163586568, 0.9163667841051334, 0.29480941987550624, 0.4224119806874219, 1.5511736406866146, 1.2598011040470545, 0.20922139549046254, 0.623351379561383, 0.33968288634213234, 1.0215867356182784, 0.642167431419222, 0.8241427079072874, 0.2606002221635653, 0.5481757938723189, 0.4368599047200722, 0.19094649507343758, 0.5155980026677058, 0.20409071817817867, 0.8222438594193857, 0.13081337723986525, 0.5663195355073043, 0.7808861242786532, 0.3376699103933321, 0.6202946597982792, 0.48613943805637977, 0.950700315106377, 0.6560624183071978, 0.4789517266938512, 0.49367126120592164, 1.1739646543635505, 0.6058275435884635, 0.12272627534966839, 0.9379765020858555, 0.3790656656319995, 0.3344458414826095, 0.6129256852796876, 0.21886416771539896, 0.5920360192061006, 0.16232869291450616, 0.8791958058483504, 0.8589138040478339, 0.21226733262827413, 0.8538228229405274, 0.48244524777094905, 0.25388355714155886, 1.421116497266616, 0.9084266411722381, 0.3698594157933226, 0.33841073021974094, 0.16638236603345735, 0.6765690085288364, 0.6620502010671054, 0.6372673259816365, 0.24190757888141415, 0.7369477814926788, 0.5967844664590694, 0.5496916943580058, 1.0015504899257912, 0.6723718870793207, 0.8228757646425011, 0.08083322197257357, 0.20919539621719685, 0.30402265151946445, 0.04761516808088386, 2.293948491025459, 0.6145232357351686, 0.22196188979301, 1.4236282582267445, 0.20904400123037425, 1.3170843182578194, 0.17147551869430794, 0.4815043081670992, 0.24494665939989863, 0.6501620791997346, 0.8895800278624417, 0.33028979586528867, 0.5795515698533017, 0.11449804268790287, 0.46400551895235254, 0.4868844066347498, 0.4375801141944032, 1.0442731427588179, 0.4016056941519122, 0.8502652251178211, 0.6092488426431119, 0.16629303833537282, 1.2397383004195661, 0.30370338733503377, 0.564634746135416, 0.4268910857580661, 0.47806740754741467, 0.3461746747562846, 0.38513425474507, 0.37377011672633725, 0.716013404086053, 0.316496496170428, 0.4188887092116073, 0.2737726921034544, 0.27938414546875306, 0.2932103143211134, 0.32954100098014877, 0.5154023889228596, 0.11494529797371869, 1.2961076002329184, 0.41888511406823453, 0.27057965265015954, 0.14843285267600237, 0.6552347382950229, 0.529840083767658, 0.505154795990772, 0.27220540422925665, 1.5743250459161717, 1.3034044878283666, 0.27865322451860525, 0.40875359578876863, 2.1498322857961334, 1.1930781568002211, 0.29280648694863176, 0.3813959282211632, 0.6998245335099427, 0.15965354323275688, 0.5797743464359678]
# stick=[0.7355900749166732, 0.28347519967213364, 0.5901538603498993, -0.18748271524744065, -0.04094174760784048, 0.42587456175037136, -0.04198540373917262, 0.3882315373066948, 0.4544906081010165, 0.13178534445885637, -0.6500664437700402, -0.1666382684955672, -0.04059153264409513, -0.4194654715152421, 0.47119377839484394, 0.23140046280793305, 1.1684527501399222, -0.2714648097608786, -0.11627605455509613, -0.08818373301929633, 0.21316081343642368, 0.11438594306482225, 0.4009689828878428, 0.20864429979874097, 0.42251743624377264, 0.06898429923802776, -0.10139442796873242, 0.36535999014164605, 0.1256411161016482, 0.26346999039256397, 1.5902002048201174, 0.6055363043099, 0.03659894369974727, 0.318566936661062, 0.08669912413394953, -0.11776364552070999, 0.37792058601477657, -1.2855645045622288, -0.13370537185612885, 1.1914495651927264, 0.7190804788826678, -0.1694345241904217, 0.03687628138822657, 0.17034851018866737, 0.6843684191860429, -0.0751519736025528, 0.34617107863436813, -0.7813999397942566, 0.4794387415757787, 0.0817367990233406, 1.4986735216133453, -0.24258158150914683, -0.48537800676407783, -0.43813108150801827, -0.008121744168173302, 1.069108582764465, -0.5059173024988115, 0.5308434420679075, -0.4089606438109287, -0.5240923217694, 0.6546827241974993, -0.13331553539681357, -0.39291074709274987, -0.2124985671160077, 0.607819434150526, -0.13256731931499588, 0.1130544340327303, 0.38273213396831257, 0.3239492541504249, -0.02667375951456386, 0.3525448774903048, 0.5142600528010506, 0.11595891436941302, 0.4423035009894265, 0.22820355528951036, -0.4530755097085429, 0.0823838127648871, 0.017267655059007198, -0.15627921115458326, -0.06640101419365489, 0.12960599261340788, 0.05793603310123953, 0.260851765979219, 1.7516317964677446, -0.10138782781132469, 0.20110867776025548, 0.036838704408373175, 0.0033918295471196285, 0.3770513775952705, 0.5557477056881552, 0.005323074444171143, -0.02867437941624614, -0.7307251072918806, -0.16570158564293952, 1.0656564924852303, 0.19248990112250555, 0.2642344272638814, -0.22661543674512558, 0.056720284701587215, 0.4781944055891346, 0.2688549871781983, -0.2876399955311127, -0.37863355318534997, -0.18716122166854335, -0.46766223071896934, -0.3357732336532991, -0.8562429345576403, -0.3883391872112941, 0.820796953735309, 0.2648765502990487, 0.18922665727288177, 0.9300082897981767, -0.19482885274930883, -0.06433449911235246, 0.47489131096808335, 0.3593028601845057, 0.14847712964961174, -0.17635613795334093, -0.673348443711843, 0.43250751636538887, 0.5125961354833817, -0.014305134830411629, 0.3091143692696692, 0.0851922866537468, -0.6112876850839147, 0.4298436370925314, 0.7629865950736068, -0.2578899326349443, 1.0085121626888047, -0.17906783862918654, -0.3152479090481274, 0.7086543498574231, -0.5453864295691883, -0.003057075331752332, -0.13804699691303718, -0.04905273198166353, 0.2645979734359134, -0.37581747878348126, 0.27347778286961394, 0.20117372396513214, 0.06056442687956964, -0.11116540878137217, -0.17966626910033578, 0.8131673820560367, 0.6673213933323978, 0.140957124089208, 0.26061466373067144, -0.4935396784017202, -0.3071275568746537, -0.30233594324487745, -0.5064183623277142, 0.3206093031695644, -0.1076668250650265, -1.4497698396169076, -0.4912456569502868, 0.6327630207814804, 0.9144989635177129, -0.24693213908791165, -0.017390307525302563, -0.854942332830799, 0.20513379623830202, 0.8064690478647841, 0.2612473063668027, 0.40306720553026393, -0.18406716913730164, 0.1528720111175038, -0.1800744848009502, 0.04268101554628136, -0.6066627436439017, 0.3509588070666559, 0.610560389882866, 0.399691609578185, -0.01955998681933443, 0.8023786016191905, 0.35747451981844824, 0.9253285371147579, -0.15770419222225912, -0.5180828476774838, 0.024168587029761466, -0.20140120374408818, 0.739457345185417, 0.5871812422418108, -0.842149593471299, -0.7784153869716726, -0.34554892720398295, 0.20467014267053807, 0.7647728666450139, 0.04758511246562415, -0.2684460127826338, 0.6496362735062112, -0.08455517074435949, -1.0949653636792402]
# betas_rfi_reward=[0.6375325825294186, 1.6014994920215941, 1.2379558820510754, 0.5396693921214003, 0.6862024673723589, 2.182939300831484, 0.12127366072389041, 0.19662351060755914, 0.07244045416595517, 0.08462288311517939, 0.8352938127154566, 0.13230719367035815, 0.21158953360514166, 1.2669305469695256, 0.13008183111643093, 0.25813310536958556, 0.3356934884396301, 0.19542922401001658, 5.110867293749371, 0.357657660593038, 0.43543554799046136, 0.3548154817235009, 0.4599278986408816, 0.7773102821305816, 1.2558424761827423, 0.19660229583665637, 0.17176527112976311, 1.6734139022338885, 0.7801921415216249, 0.9354764469208335, 0.8170372731145941, 1.8617477519710763, 0.386293290067782, 0.5574756483577796, 0.09883842956670894, 0.2717216175008933, 0.5935969958553082, 0.4571695171047425, 0.7898281323756077, 1.0973947189848807, 1.2840386310793823, 4.144447824558218, 0.40453226823058425, 0.9807528313869052, 0.8216188122534549, 0.1220641180570214, 0.06684430118410432, 0.2059651118460156, 1.8583540775424015, 0.7080046643300645, 0.9253241096507394, 0.3775840515455995, 0.2274476325371943, 1.877538639727384, 0.42304967827091655, 0.448744038094037, 0.5516689850759183, 1.1443865013394388, 0.48887943670321377, 0.5837447444607737, 0.6168512990778643, 0.5058357784428645, 0.3726685538706609, 0.3747916592179852, 0.5072255852642736, 0.6154853807091705, 0.3881440309309507, 0.5356197917477763, 0.4627256152602418, 1.9043338872470696, 1.8582266767679707, 0.5081947599424086, 0.7644489228219876, 1.794562417488205, 0.6760096013998866, 2.784722172177169, 0.3472691945030932, 0.29508009244079636, 0.4732929510962687, 1.5002833850069586, 0.37588046253260365, 0.5780437031418987, 0.08857790359269628, 0.8516402760814967, 0.5625378819138044, 0.4801514057509091, 1.2312167940517025, 0.21763755256490383, 1.6748570000964684, 0.05605365323826296, 0.5615657414090008, 1.3782008911942663, 0.3264883184553961, 0.06780370946824385, 3.729426563179735, 0.7962379134017844, 0.7665966112793507, 0.6405719830548222, 2.271730003074904, 4.164289406853134, 0.5611355812051322, 0.0999086556370807, 0.43846574539126043, 0.18487134050479254, 0.5076552870555601, 0.28789643850940605, 0.11822300547522445, 0.4484635830830648, 0.2687952677138732, 0.7322489771095705, 0.8804762274669826, 0.0931269962050736, 0.27247611764134577, 0.3795211835743172, 0.1974343677512305, 2.487380848170638, 1.0908444992533342, 0.16544534574262582, 0.4947628425385415, 0.2450630476830991, 0.2915058660312281, 0.9862650378888048, 0.36312618150762616, 0.5893324665512422, 0.6620873358956886, 0.6583276656209023, 0.3569809379410897, 1.1557437934234296, 0.6019635319324902, 0.26977851655570895, 0.21262265264317806, 0.3196013528228155, 0.37536705405968995, 0.09481265369357784, 0.34039810589928143, 0.7712424249132631, 0.17327575076372034, 0.6365438319144917, 0.26570982522220343, 2.126620165605291, 0.32607947403018284, 0.13604785583952136, 0.4050546643240517, 0.3877560448636367, 1.3806716212775647, 0.40326258579807694, 0.4954460676631143, 0.10921415578852672, 0.746546592303754, 0.19920878426392968, 0.34551142767743515, 0.3457259202996614, 0.4057522290436298, 0.5383641269721388, 1.0779415251916717, 0.5439187477082388, 2.968328304768259, 0.19594057751117153, 0.3516140619888852, 0.4079877695830117, 0.19345782130575548, 2.394833002619781, 0.48937348996247776, 0.21269130806589454, 1.1807699582820768, 0.36874114418532844, 0.14477690599251372, 0.10815509191772493, 0.46614846181034486, 0.19327522013571669, 0.207531456134537, 0.22759929166589954, 0.08090689882076951, 0.5309106054458897, 1.9255203961511822, 0.822574711915923, 0.074151969880939, 0.5219589256332697, 1.8741090468875197, 0.5080203927175327, 1.16140144449635, 0.4595392982271129, 0.7883712252683203, 0.5810302728273206, 0.8915283078256234, 0.4866123256588683, 0.2535005897516684, 0.38704979432196396, 0.14581305130815472, 2.7231696415167455, 0.4228396556553876, 0.594323860185794]
# betas_rfi_predator=[1.7962066035287103, 1.0252175613598324, 0.3820587658144291, 0.5702205792972918, 0.5169091328583498, 1.7626440458592108, 0.20842326129720812, 0.6750722519047205, 0.17647299696839328, 0.574506374623047, 1.887249327441629, 0.8663866211792346, 0.43788322547001335, 0.8164538764580911, 1.2413175289021865, 0.7479182586669499, 0.9800765869439827, 0.969190434377163, 0.2981525084636172, 1.3502248196093014, 1.403449041246343, 0.3732445158126646, 0.5518578937514048, 0.8059881223658141, 0.7079483542396892, 0.30608235457429583, 0.9750775705585997, 1.5879598573793805, 0.644380738362182, 0.8626325474360969, 0.829900010635984, 0.5086883216713476, 0.636552344814039, 0.5447349730481943, 0.6110601308081951, 0.4815008282600608, 0.4963682074882467, 0.8261301506125757, 0.43382975709713584, 3.241040344067834, 1.5658403139586663, 0.7457985546687029, 0.16140222636679746, 0.7555692213025466, 1.04772694945781, 0.37567027002049147, 0.6121326654598068, 0.7079753413865157, 1.6111582875266122, 0.670795264780941, 0.5628129491670177, 0.5128239807917067, 0.5915101408688308, 0.6261432585984179, 0.6516843701121475, 0.751825517256567, 0.43754788830209806, 0.3088315915889896, 0.6849141363708877, 0.6604441574180676, 0.5131590160753582, 0.8959420899433854, 0.6042717862110013, 0.6476612275099572, 0.6509050327004776, 0.6835138402824105, 0.8557362682231319, 0.5421112963980497, 0.8652844136730723, 2.634913365688407, 2.0218603659314436, 0.548871480789398, 0.5272727331321961, 1.7558975986819791, 1.116907556756564, 0.8371669294782752, 1.0310692895696278, 0.7701559706171762, 1.032260377596261, 0.6537646408567562, 2.15880395054399, 0.9861303658355409, 0.09289751688379776, 0.5075206829720967, 1.1628434198006348, 1.2911922902682476, 1.3580971299838114, 0.4158851216826408, 0.6430513126962913, 0.168906254987244, 0.8526456516591855, 0.1682813512313747, 0.6153183085942595, 0.15628922633355236, 1.4691178184184397, 1.6786817496814772, 0.5323056952032007, 2.6706879271636677, 3.924790619946508, 0.8439782378278374, 0.44647877319680085, 0.8854979285982812, 0.6243870158819249, 0.27388006740668186, 0.6400725805956856, 0.6988328500984381, 0.2926258447660078, 0.2747009971166461, 1.0066827651202799, 1.8608830823227116, 0.8808760293466403, 1.3868704374785086, 0.7090174803391626, 0.2997981085185871, 0.2924261279058482, 1.1378644262156354, 1.7033750878853353, 0.3433028112313259, 0.6606451649499294, 0.8317245293701558, 2.50094161974571, 0.8830730770660907, 1.346158212913694, 0.5230590427070747, 0.2575479331045076, 0.4530177869562505, 1.658300366972868, 0.3409792366064464, 3.089624859652677, 1.013949928152006, 2.564982186706508, 0.8076387968519192, 0.8621131576901588, 0.5753986387311023, 0.7288365813217303, 0.3046767197909037, 0.4121191161753852, 0.6269371933430233, 0.5041025027959876, 0.25008685909765277, 0.6019438479827776, 0.18619980493399013, 0.7088198122888781, 2.0733382917634606, 1.5740878350283758, 0.5506468381889217, 1.3541563550512759, 0.33384296638758915, 0.6893427391796408, 0.768049078562313, 0.40139365780617403, 0.77136567097499, 1.665894853038164, 0.5326186905851891, 0.32642117448998664, 0.08618053470919632, 0.3310520236285575, 0.43567178858000255, 1.355025975802155, 0.6373720894364017, 0.42334604230552897, 0.2026709754734678, 1.2548012192309415, 1.0495771985384985, 0.29754508022762277, 0.6651414987744707, 0.28057859500057464, 0.2561593532970631, 0.455230557868922, 2.642161968052399, 1.3350188014558517, 0.2594098693812713, 0.4455756656756415, 6.227261546415632, 0.5718063025353213, 1.7164682873512747, 1.2305738228166572, 1.0394919351045346, 1.993772509149671, 0.6775986897616317, 0.31271853517136927, 0.7234205453773728, 0.13570924271943885, 0.7365149574319511, 0.6436673884397047, 0.5810367128992219, 1.8119558864169025, 0.2609215620896984, 2.070407439578111, 1.5303609569635128, 0.7429481363235084, 1.0102170849051921]
# lrs=[0.4631581617428217, 0.05478959996938417, 0.04573870627115641, 0.020501119565291215, 0.014851745662672732, 0.07787931032103887, 0.4418939463191761, 0.18359976830766586, 0.8216139890509984, 0.3727004264119918, 0.05541180081748502, 0.23290698678274727, 0.06629377441888712, 0.0741253480210752, 0.5932277739300282, 0.019475053538288798, 0.03612361032203006, 0.044005667512471855, 0.013230781109766271, 0.06190206102930348, 0.2379229941184542, 0.7590458825326994, 0.058816240951506844, 0.015463228656923365, 0.07495485702280255, 0.12940056065408437, 0.2796794469454237, 0.3515492444904585, 0.12063720789506964, 0.04128352215599898, 0.5378071565319419, 0.4096025642681578, 0.018347486752203757, 0.0788686906154935, 0.17147436227391222, 0.39656235407186236, 0.10759275591001785, 0.02638696463037676, 0.008892656565214629, 0.14001471506698845, 0.024706454185564573, 0.5272777961131142, 0.12145641420660751, 0.025092777270995775, 0.1425564926102784, 0.05510807435159929, 0.1614009221495198, 0.4028187734752178, 0.6073059058032293, 0.20432634629528768, 0.03636878500672913, 0.24345532245692805, 0.5164384837900622, 0.578784646154188, 0.024960339215485218, 0.021163333587064317, 0.06162154160544525, 0.5253062876194208, 0.008274776580473302, 0.00902099175029322, 0.019946438250837195, 0.06532358662375572, 0.21025112251105685, 0.30805504444691056, 0.0529554349462132, 0.02696765803928044, 0.8922982117990746, 0.04992175966793885, 0.3929399444160031, 0.4445725012824234, 0.726966402144016, 0.057469349801433776, 0.0305542558425844, 0.5645068716743987, 0.0925148939656612, 0.014162041920266974, 0.027856281623912946, 0.04578715221823025, 0.01681414795913168, 0.03420030468795044, 0.1552012561000144, 0.6554974964312129, 0.14822572593524053, 0.12379000707604927, 0.3119540934095823, 0.030492491125135114, 0.026630714711881737, 0.07809646751786399, 0.23826272022615522, 0.08662193402658717, 0.15663546056390923, 0.6493756445184844, 0.2099061163641076, 0.01442309400098315, 0.39554047882308324, 0.08322184971025072, 0.024107637629020837, 0.01823403128950293, 0.05534715139503482, 0.08824362582501416, 0.2414828490043559, 0.011510300322487193, 0.06867400169737922, 0.1479500469955878, 0.050323074444184185, 0.05605015147962926, 0.031572886916608235, 0.3351517153085931, 0.019484894891413612, 0.2228486027316401, 0.4353529881836679, 0.43667551254172293, 0.015736169262090007, 0.6419558456672829, 0.10868659662596157, 0.2784166489121651, 0.07462708262972832, 0.12135070972748606, 0.04225027846058551, 0.030513979909924046, 0.5544235914520682, 0.017972780261585112, 0.11390523947488086, 0.04839752100689489, 0.08701932300106115, 0.07941314353184072, 0.10286889282608334, 0.36875946165982426, 0.022346231181866495, 0.14127780062693107, 0.3347014062998585, 0.01940903301800724, 0.015416559241209903, 0.021146456299089254, 0.015466005639019453, 0.6633609151356873, 0.13435526951321913, 0.22188684662531433, 0.05799877662425233, 0.02734731022770045, 0.06973259870720595, 0.03398113847376706, 0.02252480442006621, 0.7057979798037504, 0.27659379005141327, 0.08199198478305565, 0.03620030535213724, 0.17019850877728798, 0.4079799005050658, 0.42000951699014705, 0.020928759408962917, 0.1326621633027396, 0.023807742817142177, 0.01708627206467448, 0.11426135882710449, 0.8396567292022319, 0.09567763677993575, 0.06568230668871443, 0.31194991360616126, 0.0961900911613661, 0.09995316774171552, 0.05152523535177578, 0.017082049831539162, 0.27239908716714034, 0.45369282681395035, 0.6234212575339061, 0.3247432491567277, 0.1176989616558006, 0.03689608335096438, 0.24885271796156122, 0.5209385183615035, 0.43780034017006864, 0.1501719138937136, 0.011777305093388977, 0.028741404887067456, 0.03751141606279069, 0.5577812376827541, 0.027282834043166542, 0.698217421231289, 0.015409575899056166, 0.11623793374111469, 0.170251966341303, 0.26676923089893606, 0.025576786239230313, 0.0262092983668189, 0.03602985038216676, 0.6919874021594161, 0.0338222104611681, 0.19661973147035997, 0.05775314298172179, 0.02394711383718237, 0.05919192272385744]
# lr_cf=[0.4011302591264488, 0.01802415720067539, 0.02908015250314487, 0.02139875254747055, 0.036432635779102586, 0.10736996189470084, 0.511238217655947, 0.21942511899490588, 0.65935250711555, 0.44011763530936865, 0.20679277408178753, 0.05141431806699821, 0.10855406436547588, 0.05060841549088634, 0.23857679523192793, 0.13995502935275492, 0.001775530997184703, 0.03550437209796015, 0.0005627162497276683, 0.0022725805870671303, 0.3472538819831831, 0.7805851103124294, 0.016175166715498634, 0.0030853359453528184, 0.009860567101167905, 0.1901445119102365, 0.03766030953770295, 0.4040204099308875, 0.01750435508981524, 0.06258819490958273, 0.06494877222905024, 0.31728894138150204, 0.012332910749681467, 0.010688120563131166, 0.1389873767706337, 0.38291134302513213, 0.041001458568186204, 0.011425709831092463, 0.0041548527191099735, 0.07829930932706386, 0.05234983798360972, 0.7205995864116386, 0.06980648686075751, 0.025861415833267926, 0.007158179384057003, 0.06500126990350731, 0.03715298691177459, 0.2303644283412543, 0.5347321012536762, 0.17774359003762777, 0.008935057252327754, 0.03801915385688858, 0.16056183390707554, 0.2742436082974491, 0.023177808675058954, 0.015446829180374036, 0.2511609586488771, 0.001822329901574626, 0.005522373179208994, 0.006373681085805912, 0.04078505612121887, 0.018437327129187747, 0.07523623312449568, 0.25852651186009995, 0.04004177746407048, 0.0012400797441948321, 0.2063060532318313, 0.03675557753696097, 0.42321040763417594, 0.547809394390712, 0.5086198437425906, 0.027659453349239752, 0.03217560239166271, 0.03790029448312449, 0.0056376160732547545, 0.0017090526992500873, 0.002984946663033195, 0.05006582414530684, 0.008099197155872151, 0.014341890292289508, 0.011810918837424229, 0.3107599052130465, 0.23854992197504649, 0.03244933502103551, 0.1763615978890258, 0.007652361522332172, 0.010557271483055523, 0.2660616915502841, 0.04189019918453595, 0.004353245847395591, 0.046605719123658844, 0.7018675702597851, 0.0419348606143545, 0.004410518293980538, 0.2804323330353617, 0.22494784889249372, 0.025289593195824668, 0.02674536620443992, 0.016998698064987747, 0.02412315211157026, 0.025734535567095197, 0.03283822930425805, 0.062313989270264866, 0.14003212755111052, 0.015156540753558033, 0.009440832469167123, 0.08358858230317547, 0.054079146968748826, 0.0002756512349198052, 0.09483713935474594, 0.7332157523030353, 0.4647364334612689, 0.0001778562703192635, 0.08495720226690998, 0.2006894621163018, 0.12782419883306986, 0.6092836036383446, 0.12656021462479608, 0.059184433946832204, 0.004008726044096601, 0.26048664488937834, 0.37289716702096015, 0.06368963914655694, 0.02955539191595325, 0.050965289495629576, 0.026307405711764386, 0.10822649319002055, 0.18832010504226068, 0.17248186528965925, 0.020338551695847345, 0.2839350821886566, 0.002150446260862134, 0.01024652575202924, 0.0005909857328521296, 0.0035583238126480326, 0.5131236236550276, 0.07821271054992097, 0.04629349489396653, 0.04987609683830483, 0.00019443399905923618, 0.018579380171198086, 0.002063680458954675, 0.07189445820742985, 0.5947730432802355, 0.01273990257573567, 0.05126248177702081, 0.10628863693170004, 0.01610108457506161, 0.2789099886965552, 0.2927355973714699, 0.02899632579337092, 0.139334173728728, 0.0006671422332970153, 0.011092779829223924, 0.07282168949378474, 0.001881724639674952, 0.0022386122747224146, 0.0023434314854496817, 0.04069786509595044, 0.024542019844456, 0.05728688415022869, 0.001271960223024982, 0.0064379144317539185, 0.06962634125732992, 0.1383405612927742, 0.5969853050818278, 0.193975655326396, 0.027489686775039046, 0.04001441501332249, 0.07150026088206858, 0.35338106854543816, 0.45816057883723976, 0.04400103596852948, 0.12663615441996712, 0.008893548534109761, 0.002449496780910467, 0.3336695297180589, 0.05265715390471947, 0.820484737289113, 0.0012738379299264055, 0.03868728636312582, 0.10307922698212388, 0.3423262359503028, 0.07993943506687014, 0.013957252838634065, 0.09728123336484165, 0.653197783363609, 0.02869334806161451, 0.3062992399080991, 0.001757174066751919, 0.007624266368369107, 0.033612666709157135]
# lr_val=[0.871833545510129, 0.5874702406173942, 0.795515648860788, 0.39871989322510354, 0.4493302775594723, 0.6946742987744573, 0.12436786498201345, 0.4631041418373784, 0.5468236007686837, 0.7580634256528899, 0.5287171590531314, 0.49072233473508636, 0.3076835843987794, 0.5537790871945102, 0.33765521459894404, 0.4842729054490343, 0.6385224021579035, 0.6451590202561518, 0.391923770365522, 0.17349037361989045, 0.5882043579515468, 0.9774210547972664, 0.498166802526189, 0.739638801759072, 0.8095411538433365, 0.6218275540654392, 0.4604701253612803, 0.5731656054926011, 0.515090920073609, 0.4304305248423239, 0.7478489782787151, 0.8144716199125261, 0.5429575369168573, 0.7059928791602365, 0.4919766798467618, 0.3978485339676327, 0.7630147739376091, 0.35019436566695883, 0.46734441288723677, 0.8238981811716465, 0.18577896876239058, 0.7719817053844742, 0.5373389369594707, 0.47983470373641646, 0.3892651369043202, 0.4389843039639192, 0.43692945261978905, 0.6339043070823756, 0.47817146426188617, 0.8593619975542843, 0.8289763043053949, 0.8755071429079836, 0.9413410387052777, 0.8563508500176092, 0.7484722481210165, 0.5047943726692834, 0.5774278114566653, 0.2073288206542505, 0.4873531143475673, 0.31374068874861366, 0.3900510729638962, 0.7579658111887333, 0.40839966513503634, 0.9270370326161542, 0.8103328550844349, 0.7598732996201796, 0.7607032966343298, 0.8504772481545511, 0.5669805779986414, 0.370728793052876, 0.9753157041388506, 0.9487470652068544, 0.2948862856072377, 0.30445151626301814, 0.3866811867730874, 0.9141793378788992, 0.5444075556488118, 0.4287253767321051, 0.34555978188390796, 0.712755580604634, 0.7691793122191858, 0.9292995968609392, 0.3420134145748111, 0.5683597096650448, 0.6049715413798332, 0.4882777941875029, 0.5909559013558626, 0.4979741337580323, 0.3745300136997503, 0.8308159777416674, 0.6345678978952047, 0.5320300986560951, 0.6164517741770729, 0.15304669623948297, 0.5705312929710168, 0.8274751787928625, 0.651633376103786, 0.5266411180596656, 0.6363802073029745, 0.5869427296817111, 0.22506621027475193, 0.6054696129923038, 0.3371097352469315, 0.5142845520451393, 0.43537424358800236, 0.7302869348342236, 0.44167096141430584, 0.5386796875157017, 0.2868527192193608, 0.4588797446090855, 0.5368432922924822, 0.39437636345344823, 0.8122436463524945, 0.5720377604827238, 0.629557325500274, 0.4892113868180843, 0.7984856102169348, 0.714784153128978, 0.47543786189293197, 0.45021673548814023, 0.2764065712307169, 0.7615907984746488, 0.2871340803347814, 0.4743409846587291, 0.7705834260765999, 0.8313097521591031, 0.6492809748406989, 0.5455781369986923, 0.2612136098221274, 0.6546961480249898, 0.46000707966349474, 0.6406989318224585, 0.44417416328459114, 0.6171388384103235, 0.35857547441891857, 0.7097381889709802, 0.2991004370138687, 0.9276491828067535, 0.4439474314992623, 0.20677712660207198, 0.569441710684927, 0.19096933712673606, 0.48945347423912644, 0.5583213661418448, 0.6742406613742006, 0.6221639815148363, 0.519524430919481, 0.3489537844668765, 0.5981940435338031, 0.7969839583144682, 0.5675436675611362, 0.5525829888967531, 0.7262462147090306, 0.655942514635356, 0.5992622894095421, 0.652319114946019, 0.6651493714587762, 0.17329948916616508, 0.7644265145336329, 0.5713861962746495, 0.5692806888008142, 0.6063645573121825, 0.4904120961127597, 0.5287606114944934, 0.6006142029520914, 0.5886751832591341, 0.21747061068530527, 0.3656733749707033, 0.3577640780441277, 0.16166320513347454, 0.5393944610142763, 0.6053779530235418, 0.26553364992836737, 0.8009519578146174, 0.5667579126846083, 0.579358077604996, 0.7525116047709316, 0.6110294317616244, 0.7459342819790985, 0.6145754775092684, 0.527201655975723, 0.5990922896328171, 0.9066133999227263, 0.504917372808108, 0.7236314798165238, 0.9133946165019453, 0.696440502909979, 0.35638447462752554, 0.604704138312475, 0.7068543469201582, 0.2910927849982975, 0.6992441504495522]

import csv


def simulate_one_step_featurelearner_counterfactual_2states_2iRF_sticky(num_subjects,num_trials,lrf,lrv,betas,mf_betas,bandits,rew_func,pred_changer,rew_changer,lr_cfr,lr_cfp,rfi_rews,rfi_preds,st):
	from numpy.random import choice
	from random import random
	from random import shuffle
	import scipy.special as sf
	import numpy as np
	import pandas as pd
	import scipy as sp
#     from sklearn.preprocessing import MinMaxScaler as scaler
	
	index=0

	for bandit in bandits:
		exec('global bandit_{}; bandit_{}=bandit'.format(index,index))
		index+=1
	num_bandits=len(bandits) 
	#define hyperparameters for each group distribution and sample from them
	all_lr_values=lrv
	all_lr_features=lrf
	all_datasets=[]
	all_outcomes=[]
	q_rew_all=[]
	reward_function_time_series=rew_func
   
	#retrieve samples of parameters from model
	for subject in range(num_subjects):
		
		exec('dataset_sub_{}_game=pd.DataFrame()'.format(subject))
		#initialize variables for each subjects' data
		q_values=np.zeros((num_bandits,)) #initialize values for each sample, assumes 2 choices
		last_choice=np.zeros((num_bandits,))
		feature_matrix=np.zeros((2,2))
		
		lrf0=all_lr_features[0][subject]
		lrf1=all_lr_features[1][subject]
		lr_features=np.array((lrf0,lrf1))
		lr_cf_r=lr_cfr[subject]
		lr_cf_p=lr_cfr[subject]
		last_beta=st[subject]
		lr_value=all_lr_values[subject]
		
		#collect subject level beta weights
		beta_safe=betas[0][subject]
		beta_pred=betas[1][subject]
		pred_changes=pred_changer[subject]
		rew_changes=rew_changer[subject]
		# betas_rew=np.array((beta_rew[subject],beta_rew[subject]))
		# betas_punish=np.array((beta_punish[subject],beta_punish[subject]))
		beta_weights_SR=np.array((beta_safe,beta_pred))
		beta_mf=mf_betas[subject]
		rfi_rew=rfi_rews[subject]
		rfi_pred=rfi_preds[subject]
		switching=[]
		choices=[]
		other_choices=[]
		outcomes=[]
		q_rews=[]
		safes=[]
		preds=[]


		#simulate decisions and outcomes based on feature learner that combines MF and SR Q-values
		for trial in range(num_trials):
			if trial>=80:
				beta_weights_SR=np.array((beta_safe-rew_changes,beta_pred-pred_changes))
				
			else:
				beta_weights_SR=np.array((beta_safe+rew_changes,beta_pred+pred_changes))
  
			# participant is told which predator they're facing
			current_reward_function=reward_function_time_series[trial]
			# current_reward_function=np.flip(current_reward_function)

			#weighting is a combo of beta weights and reward function
			full_weights=beta_weights_SR*current_reward_function
			#derive SR q-values by multiplying feature vectors by reward function
			q_sr=np.matmul(feature_matrix,full_weights)

			q_irf_weights=np.array((rfi_rew,-1*rfi_pred))
			q_insensitive_RF=np.matmul(feature_matrix,q_irf_weights)
			#integrated q_values from MF and SR systems
			q_integrated=q_sr+((beta_mf)*(q_values)) + q_insensitive_RF +((last_beta)*(last_choice))
	  


			# q_integrated=q_sr+((beta_mf)*(q_values))
			#determine choice via softmax
			action_values = np.exp((q_integrated)-sf.logsumexp(q_integrated));
			values=action_values.flatten()
			draw = choice(values,1,p=values)[0]
			indices = [i for i, x in enumerate(values) if x == draw]
			current_choice=sample(indices,1)[0]
			
			#save if participant switched
			if len(choices)==0:
				switching.append(0)
			else:
				if current_choice==choices[trial-1]-1:
					switching.append(0)
				else:
					switching.append(1)
					
			#save choice   
			choices.append(current_choice+1)
		 
			#get current feature latent probabilities
			
			cb1=[x[trial] for x in bandit_0]
			cb2=[x[trial] for x in bandit_1]
	
			current_bandit=np.array((cb1,cb2))
 
			#get feature outcomes for current trial
			feature_outcomes=np.random.binomial(1,current_bandit)
			safes.append(list(feature_outcomes[:,0]))
			preds.append(list(feature_outcomes[:,1])) 
   
			#concatenate all feature outcomes into single array
			current_features=feature_outcomes[current_choice]
			
			#determine current outcome by feature vector * reward function
			current_outcome=sum(current_features*current_reward_function)

			#save how distant estimated EV is from actual EV 
			q_rews.append(np.abs(current_bandit-(q_sr[current_choice])))
			outcomes.append(current_outcome)
			
			# Prediction error over features
			pe_features = feature_outcomes-feature_matrix # Feature prediction errors
			#Feature updating
			current_decision=current_choice
			last_choice=np.zeros((num_bandits,))
			last_choice[current_decision]=1.0

			other_decision=abs(current_choice-1)
			other_choices.append(other_decision+1)
			#safe feature
			feature_matrix[current_decision,0]=feature_matrix[current_decision,0]+(lr_features[0]*pe_features[current_decision,0])
			feature_matrix[other_decision,0]=feature_matrix[other_decision,0]+(lr_cf_r*pe_features[other_decision,0])
			#predator feature updating
			feature_matrix[current_decision,1]=feature_matrix[current_decision,1]+(lr_features[1]*pe_features[current_decision,1])
			feature_matrix[other_decision,1]=feature_matrix[other_decision,1]+(lr_cf_r*pe_features[other_decision,1])

			#Model-Free action value updating
			q_values[current_choice] = q_values[current_choice]+ (lr_value*(current_outcome-q_values[current_choice]))

		#save all output
		q_rew_all.append(np.sum(q_rews))
		all_outcomes.append(np.sum(outcomes))


		exec('dataset_sub_{}_game["rf"]=list(reward_function_time_series)'.format(subject))
		exec('dataset_sub_{}_game["preds"]=preds'.format(subject))
		exec('dataset_sub_{}_game["safes"]=safes'.format(subject))
		exec('dataset_sub_{}_game["chosen_state"]=choices'.format(subject))
		exec('dataset_sub_{}_game["switching"]=switching'.format(subject))
		exec('dataset_sub_{}_game["reward_received"]=outcomes'.format(subject))
		exec('dataset_sub_{}_game["other_choices"]=other_choices'.format(subject))
		exec('dataset_sub_{}_game["switching"]=switching'.format(subject))
		exec('all_datasets.append(dataset_sub_{}_game)'.format(subject))

		
		
	return all_datasets,np.mean(all_outcomes),np.mean(q_rew_all),choices,safes,preds


##### GENERATE SYNTHETIC DATA

# #### import pandas as pd
# #assumes you've generated bandits variable with the block above
import numpy as np
from numpy.random import beta as betarnd
from numpy.random import gamma as gammarnd
from numpy.random import normal as normalrnd
from random import sample
from random import shuffle
import pandas as pd
# #assumes you've generated bandits variable with the block above
all_sim_data=[]
mean_differences=[]
mean_differences_log=[]
t_differences=[]


df=pd.DataFrame()
num_subjects=192
num_trials=160

lrv=list(betarnd(0.82,0.49,num_subjects))
lrf=np.zeros((2,num_subjects))
lrf_safe=list(betarnd(0.32,1.32,num_subjects))
lrf_pos=list(betarnd(0.6,2.0,num_subjects))
lrf_neg=list(betarnd(0.4,1.2,num_subjects))

pred_change=list(normalrnd(-.15,1.01,num_subjects))
rew_change=list(normalrnd(0.50,1.10,num_subjects))

# lrf_mild=list(betarnd(2,5,num_subjects))
lrf_each=list(betarnd(3,5,num_subjects))

lr_cfr=list(betarnd(.18,1.23,num_subjects))
lr_cfp=list(betarnd(0.40,1.2,num_subjects))

lrf[0]=lrf_safe # learnabout safe feature
# lrf[1]=lrf_mild # learn about mild feature (small reward)
lrf[1]=lrf_safe # learn about predator feature

beta_safe2=[2.2689329354703864, 0.39962986909585496, 0.37438395554775167, 0.44045989069667163, 0.2964286390628634, 0.6789728500355544, 1.0672085702440943, 2.21711337988313, 2.2223158179445983, 5.140345191342408, 0.7139650432803486, 1.1877218440898742, 1.659662604460724, 0.9703270406711743, 1.6462084593031363, 0.3294818079180285, 0.7247488158972384, 1.2530454033702627, 0.4104537756325295, 2.5575703783053734, 2.106811301824626, 2.077845357613939, 0.5791493712961916, 0.3967301224075789, 0.9395255035219369, 2.5531207945506673, 1.340332258426472, 0.5917673407735833, 2.6406093535457416, 5.018601627282687, 0.8541580166071906, 1.1449898269082472, 0.17224991188726038, 0.3756220870020297, 3.750698494042495, 1.3423657506241329, 0.3354740331130716, 0.4134059916985107, 0.2411837392268351, 0.734584767560621, 0.5196016771358305, 0.6736943938872781, 0.7897298872399329, 3.8933480847954285, 2.8441635729298462, 4.336412798267449, 4.243409212158712, 1.481662740140216, 2.954288949616965, 3.7406970108775144, 1.149420442247186, 2.832314854778053, 0.6862605329498175, 0.4970939020946326, 0.5309774594883226, 0.33565509436800645, 0.6972595534250349, 0.8079371577285861, 0.32892493565362035, 0.33436909593293346, 0.4196084498920836, 0.6483053579365816, 0.7314108800557585, 1.2652197469479192, 0.37523388176462213, 0.5208560377858036, 1.6756681973652001, 0.515855361145909, 0.9323181297675129, 0.880799179849879, 1.2925822558005613, 0.26767757367847367, 1.1141318691731974, 0.9241991367071007, 0.7986451960313163, 0.7176419277795384, 3.4269235187071447, 0.1604131496310732, 0.3804095390479306, 1.0876834025761233, 3.4001965609484532, 0.8735087471814378, 4.278730084306083, 0.41335179782857606, 0.7864479510965697, 3.66419782659348, 0.2901089252715895, 0.22625539267612038, 1.2188897731558272, 2.335474314232028, 2.8264507123880733, 1.9794030504684907, 0.15932850348963168, 0.695670288277682, 1.5006338598055673, 0.5030012483717614, 0.4943305714394677, 1.7021019537401743, 1.6205537825568144, 0.7164725598960978, 1.0551642983220577, 1.2569717272179233, 0.20342974180543114, 1.0553178566252082, 0.2257854508656344, 0.33626010413588836, 0.19128933470719847, 0.3996225341463816, 2.037527066204333, 1.5847134801192462, 2.6656696989244217, 3.2870920746920116, 0.3498223924482916, 0.6525131384389603, 1.6188182982481136, 2.323629258691179, 0.6352518951091861, 0.3921803339750001, 0.32593626650069724, 5.673845383274456, 1.1174949253644064, 1.8358826144148812, 2.706191407898744, 0.248813380103639, 0.9427713246984764, 0.36572754381905725, 2.9597010939420154, 2.319737323912058, 0.7747204400734986, 0.5779769585468628, 3.1762472977630196, 1.50107348721254, 1.2386200745220335, 2.009086160510588, 0.1858100688703734, 1.1519131333684904, 2.518493252347767, 0.36759008126902115, 0.2272033503995823, 0.33414572568537027, 0.5747753888561963, 0.36281302048457814, 0.2069147780437321, 2.892334464419705, 0.5902071471516841, 0.363746115552152, 4.310928749619792, 2.0975690199029566, 0.23150377283981757, 2.1787900723486757, 1.8012090463192703, 1.7424733310853084, 1.0930944589428377, 0.332210153368242, 0.5913460247141507, 0.6385261777949478, 2.234883741714161, 1.8341837247454604, 1.7316742348904661, 0.5090990016977976, 5.660777514501875, 1.381744716185705, 0.2567276648136703, 2.1450346086709327, 0.4071525984852194, 1.4858094937293476, 2.8457256325382847, 0.47079449603012885, 0.817357489395996, 3.901242044723324, 3.031157434998651, 3.6985283928538344, 1.974158102762095, 0.7455134591824567, 0.30686441980846485, 4.379727910451807, 1.7649533349465942, 0.438063285896065, 1.017508422889, 0.3547703229049962, 3.0317430131780654, 0.8746437312954862, 1.1334164359377494, 0.4214681463884565, 0.3763696957710922, 0.6856169265875851, 1.8419499242132622, 0.33523738644685314, 0.5771461084839957, 1.3344589583847266, 0.3926466917887068, 0.5058664104187638]
beta_pred2=[0.624512345662483, 0.7084556998369701, 0.29698828534229044, 0.4069458277322293, 0.3139014063289623, 1.3503976640183895, 1.0014003795211246, 2.7277583113882833, 2.6692516215006004, 2.993368991093062, 0.949835959789194, 0.5201438315842977, 1.326491913405545, 0.9687487173105112, 3.1320193453533283, 0.36532140654721473, 0.9764172356003784, 1.068103314399673, 0.14959029412740946, 1.3302197257392807, 0.5345605640044002, 1.0456949131374815, 0.2885182354647527, 0.4136897676794111, 1.394183789359169, 2.6225171897564916, 0.4974719249797686, 0.9822133263981945, 2.9567597998806057, 1.2334101940431217, 0.6072018758081198, 0.850511745359801, 0.5343668487243367, 0.23634929446714997, 4.33465487778218, 0.6963736259369832, 0.3932804497616963, 0.6996726960045684, 0.3350566680752865, 1.6557478608687448, 0.40554867044609805, 0.8880699148844999, 0.5709218832481169, 1.1805686924052838, 3.1170509051536244, 0.2295827453627304, 0.95428801029299, 0.2379768190103232, 1.0840980208020738, 4.266115945596999, 0.8703473091646802, 1.2193089828519166, 1.5302658426041762, 0.535539601362688, 0.3930502541507427, 0.385807751409635, 0.788734038352794, 0.4517952495707459, 0.25786083879063804, 0.2240335817268494, 0.24876612012346627, 0.382890397167128, 0.2601078571870593, 0.7588700141793642, 0.24115659584304913, 0.7033697836871526, 0.44012793701330943, 0.4646468348424612, 0.8498339598906087, 0.23283984572586736, 1.6938244135931106, 0.5211675407074309, 0.42881206480943274, 0.775620296727159, 0.849956765231514, 0.5848592434018736, 0.6452447742948818, 1.3333646816439075, 0.3552309560081448, 0.44565485946321337, 2.1060287385238095, 0.32744047643457375, 2.635466917433532, 1.0056734469193918, 0.3532185200757274, 0.9915889766928537, 0.3182652345062319, 0.6067759637079745, 1.2513416017170793, 0.5569398337872074, 1.6274974749949136, 3.447380343000604, 0.5759956539685726, 3.490651677037624, 0.6770028734413853, 1.122552527512066, 0.19307467622461497, 3.669318102616943, 0.5862163591959728, 0.8534453755970456, 0.4736013239529319, 0.4547598566974036, 0.8435060521819008, 0.3288060274218774, 0.5311463525913681, 0.8597237662902343, 1.9071209501019113, 0.21842407103264105, 0.16814152464686033, 1.7683136970115527, 2.535616081220579, 1.5236479875899676, 0.2503840035127814, 1.0296099497865048, 0.30425409163503503, 1.6544061529920773, 0.7175380763349931, 0.8502799732694718, 0.3925354681695081, 1.2497056116820986, 0.6494674872043527, 0.39805823501211085, 0.7126382811015869, 0.27496721322324386, 0.8024513212024631, 0.4042517069983663, 1.1996273612453638, 0.5684450594571315, 0.7942668879514613, 0.8470911515990396, 0.6027922005865074, 1.4075049450710497, 0.21552928176760908, 0.5931458468993929, 4.015155674518678, 0.6522406705846984, 10.522233407837055, 0.2737351051097283, 0.37021976691937075, 9.096775311017323, 0.21554459546150673, 3.5804359463689157, 0.7790082136216336, 1.9893666956774738, 0.6687251995434883, 0.5515065083603489, 1.1826962306411044, 0.6710791254019282, 0.5311294835629522, 1.065990738288845, 0.3806791112471364, 0.4449326539491817, 0.41829978971716414, 0.4042194544156026, 0.3504252643300369, 0.07498115793189589, 0.8484027302531589, 0.7131798424553094, 1.1883316481109378, 0.2550681595299639, 5.000985685953606, 0.3374766311349366, 0.4918026207609458, 0.3121997868324975, 0.3791217855982946, 2.6653396555894413, 2.2150174485837635, 1.3718102106032946, 0.4824053570337052, 0.6819946196645109, 1.8250791400195618, 5.685365578853788, 1.5322424747608643, 1.6078100254390044, 1.101240690442118, 2.0829342974385177, 0.8309987522473892, 0.48365312436864005, 0.9053614976647273, 0.40572022426857945, 1.5357363868674925, 0.3980631489642636, 0.3382141190955721, 0.4876232665548604, 0.5537289850867952, 0.5422763118390861, 1.0009106084336823, 0.4310790706684289, 0.46935077641094847, 0.709290991062935, 0.4861973504494487, 0.5239739529634144]
both_mb=beta_safe2+beta_pred2
beta_safe=random.choices(both_mb, k=192)
beta_pred=random.choices(both_mb, k=192)



betas=np.zeros((2,num_subjects))
betas[0]=beta_safe
betas[1]=beta_pred
#np.zeros((num_subjects))+3.0 # learnabout safe feature
# betas[1]=beta_mild # learn about mild feature (small reward)

#np.zeros((num_subjects))+3.0 # learn about predator feature

#for beta model, there is only a single learning rate over features
lrf2=np.zeros((2,num_subjects))
lrf2[0]=lrf_safe # learnabout safe feature
lrf2[1]=lrf_safe # learnabout safe feature
# lrf2[2]=lrf_safe # learnabout safe feature


it=list(gammarnd(1,1,num_subjects))
mf_betas=list(gammarnd(1.10,0.54,num_subjects))
# beta_irf=list(gammarnd(0.29,3.92,num_subjects))


rfi_rews=list(gammarnd(0.42,1.68,num_subjects))
rfi_preds=list(gammarnd(0.60,1.43,num_subjects))
stick=list(normalrnd(0.10,0.54,num_subjects))


weights=list(betarnd(7,3,num_subjects))


bandits=np.array([[[0.6      , 0.60805329, 0.64223715, 0.66108658, 0.63132414,
		 0.63819914, 0.66034901, 0.63861242, 0.62434063, 0.6219998 ,
		 0.62666403, 0.681965  , 0.74062695, 0.73959924, 0.73234132,
		 0.70622878, 0.70860292, 0.66918965, 0.68199129, 0.665275  ,
		 0.66605933, 0.70206879, 0.65193979, 0.67573929, 0.6207538 ,
		 0.67356489, 0.6749427 , 0.69401594, 0.69160183, 0.64106154,
		 0.60824385, 0.60511121, 0.58242085, 0.62319557, 0.6018896 ,
		 0.60125991, 0.59645454, 0.529473  , 0.54165716, 0.57075333,
		 0.57075333, 0.5062713 , 0.45564257, 0.48367731, 0.39421098,
		 0.38230041, 0.35759676, 0.36203349, 0.36962064, 0.41477751,
		 0.3767546 , 0.39871607, 0.34677857, 0.28580469, 0.25      ,
		 0.29890393, 0.3267624 , 0.29516851, 0.25      , 0.25638191,
		 0.27158113, 0.2571444 , 0.25      , 0.25      , 0.27254536,
		 0.25      , 0.25864068, 0.25110371, 0.27663029, 0.29771642,
		 0.29093551, 0.25      , 0.26463816, 0.31434071, 0.28685784,
		 0.30410655, 0.30480693, 0.37919861, 0.46427797, 0.46510432,
		 0.46510432, 0.46833986, 0.44544402, 0.39909721, 0.37956105,
		 0.40499503, 0.4617408 , 0.43821381, 0.41709882, 0.3974901 ,
		 0.39265371, 0.31190903, 0.36691765, 0.3645172 , 0.352056  ,
		 0.40998723, 0.35362532, 0.35921699, 0.35834497, 0.38741614,
		 0.40525525, 0.40697073, 0.43906215, 0.4076705 , 0.39910291,
		 0.35647728, 0.46276072, 0.42230641, 0.4582233 , 0.43813438,
		 0.47119909, 0.42486406, 0.4803197 , 0.521849  , 0.49565621,
		 0.52156436, 0.5588931 , 0.61103105, 0.63322394, 0.66400169,
		 0.66400169, 0.69650804, 0.69988327, 0.74116342, 0.75      ,
		 0.75      , 0.75      , 0.75      , 0.75      , 0.72675456,
		 0.75      , 0.75      , 0.75      , 0.75      , 0.7455043 ,
		 0.72851387, 0.73790804, 0.70809613, 0.721378  , 0.7340619 ,
		 0.75      , 0.74366549, 0.73160791, 0.75      , 0.74914169,
		 0.70856939, 0.75      , 0.74452707, 0.75      , 0.75      ,
		 0.75      , 0.75      , 0.72229815, 0.73149275, 0.74587327,
		 0.70557904, 0.70042565, 0.73559079, 0.72480766, 0.73527969],
		[0.6       , 0.63424552, 0.64856632, 0.63049254, 0.62698429,
		 0.68987999, 0.74264135, 0.75      , 0.75      , 0.75      ,
		 0.75      , 0.75      , 0.75      , 0.74203498, 0.72735643,
		 0.75      , 0.75      , 0.72761434, 0.71794343, 0.72306527,
		 0.72803291, 0.65287509, 0.63672559, 0.61255005, 0.60513644,
		 0.67243792, 0.7241975 , 0.75      , 0.69001187, 0.71337992,
		 0.70847111, 0.71181472, 0.63586758, 0.65502604, 0.67819872,
		 0.70468148, 0.71043717, 0.63675853, 0.63454196, 0.58912117,
		 0.58912117, 0.64177559, 0.62360582, 0.59046811, 0.55892549,
		 0.5776706 , 0.59290765, 0.58230861, 0.66910086, 0.72844168,
		 0.70325029, 0.67238649, 0.69057175, 0.68408818, 0.7405809 ,
		 0.73050571, 0.71480596, 0.73976085, 0.70781482, 0.70408816,
		 0.66976011, 0.60057571, 0.61687003, 0.58354019, 0.53875765,
		 0.54843125, 0.54587807, 0.57762906, 0.61732986, 0.61886372,
		 0.69537094, 0.64652771, 0.63610021, 0.66779593, 0.66225495,
		 0.63128954, 0.57116772, 0.57248497, 0.55403437, 0.53079311,
		 0.53079311, 0.57024924, 0.64967336, 0.67151795, 0.6947253 ,
		 0.72437541, 0.75      , 0.73883327, 0.73666674, 0.74432003,
		 0.75      , 0.75      , 0.74007385, 0.72177071, 0.68983859,
		 0.72085913, 0.75      , 0.75      , 0.75      , 0.71773696,
		 0.73004666, 0.72601422, 0.72528023, 0.73280499, 0.73049834,
		 0.7122964 , 0.69234932, 0.71837592, 0.64383412, 0.6512838 ,
		 0.6470011 , 0.62977183, 0.64584272, 0.73580107, 0.74240085,
		 0.73107863, 0.71981002, 0.75      , 0.71356773, 0.70550977,
		 0.70550977, 0.71066027, 0.75      , 0.75      , 0.70816558,
		 0.7218489 , 0.74813071, 0.75      , 0.75      , 0.75      ,
		 0.73925907, 0.7393377 , 0.75      , 0.72508797, 0.75      ,
		 0.75      , 0.75      , 0.70866687, 0.71605913, 0.69932325,
		 0.70412698, 0.71118275, 0.65177458, 0.75      , 0.7423786 ,
		 0.71768228, 0.73833822, 0.75      , 0.75      , 0.75      ,
		 0.73428418, 0.75      , 0.73578223, 0.72427645, 0.73363825,
		 0.72077829, 0.7198784 , 0.72271497, 0.63146213, 0.55511139]],

	   [[0.4       , 0.39715217, 0.38940767, 0.39905892, 0.38734784,
		 0.38410727, 0.36022563, 0.36727394, 0.42582955, 0.36860936,
		 0.3111515 , 0.31076305, 0.25      , 0.25397115, 0.25      ,
		 0.29002151, 0.27412911, 0.29844404, 0.26500396, 0.26041497,
		 0.30922817, 0.28732961, 0.28483449, 0.37130388, 0.38552943,
		 0.29295538, 0.32391132, 0.31710554, 0.2618178 , 0.27729646,
		 0.28745137, 0.27239061, 0.31089574, 0.25      , 0.25      ,
		 0.33908969, 0.32744188, 0.30133806, 0.34353537, 0.3264284 ,
		 0.3264284 , 0.37149691, 0.35875301, 0.40221841, 0.54005202,
		 0.60760845, 0.657979  , 0.64919505, 0.70605911, 0.71826569,
		 0.75      , 0.67415382, 0.68224385, 0.68420757, 0.67961411,
		 0.73282033, 0.75      , 0.71994701, 0.75      , 0.75      ,
		 0.73389666, 0.74488122, 0.72361023, 0.70410426, 0.69378345,
		 0.73422189, 0.72524597, 0.69632013, 0.73838088, 0.74023583,
		 0.70657051, 0.67766682, 0.64604851, 0.61363081, 0.65215412,
		 0.70704194, 0.68047235, 0.705638  , 0.73726273, 0.75      ,
		 0.75      , 0.72444726, 0.68454413, 0.74231003, 0.71816513,
		 0.69109469, 0.75      , 0.72828696, 0.75      , 0.75      ,
		 0.75      , 0.73241382, 0.75      , 0.75      , 0.73973545,
		 0.67385495, 0.69560547, 0.69879151, 0.70706774, 0.73490245,
		 0.70277481, 0.72446198, 0.71907993, 0.68131313, 0.66108024,
		 0.64181268, 0.70094793, 0.70634092, 0.75      , 0.72967419,
		 0.7150081 , 0.655094  , 0.64217499, 0.66913485, 0.66151255,
		 0.67748868, 0.67461031, 0.68807299, 0.75      , 0.75      ,
		 0.75      , 0.71702871, 0.7135439 , 0.64691024, 0.59449908,
		 0.62197249, 0.58435716, 0.52287236, 0.54432158, 0.51293064,
		 0.47987584, 0.54604542, 0.4857259 , 0.48860359, 0.45237991,
		 0.44125567, 0.41303666, 0.43191558, 0.36642387, 0.35298046,
		 0.38073481, 0.41125331, 0.35688023, 0.28314537, 0.2913587 ,
		 0.2944217 , 0.35609871, 0.33412751, 0.33571022, 0.3136283 ,
		 0.26266767, 0.25      , 0.25      , 0.30585691, 0.27257196,
		 0.32023814, 0.27259039, 0.25      , 0.36836493, 0.36003922],
		[0.4       , 0.3470691 , 0.28552572, 0.25      , 0.25      ,
		 0.27133294, 0.25      , 0.25      , 0.25      , 0.25      ,
		 0.25      , 0.25      , 0.25      , 0.25      , 0.25      ,
		 0.28825665, 0.26285161, 0.29694452, 0.28157921, 0.3097387 ,
		 0.33877861, 0.35789613, 0.35856376, 0.3835424 , 0.34036174,
		 0.3376338 , 0.35099526, 0.41345034, 0.40038635, 0.47372792,
		 0.56417448, 0.55951499, 0.52272065, 0.56038754, 0.55992294,
		 0.50500372, 0.39174961, 0.36566148, 0.37176374, 0.43961623,
		 0.43961623, 0.40433017, 0.34617404, 0.35344697, 0.35182129,
		 0.37826158, 0.40132678, 0.35515851, 0.36251624, 0.36228041,
		 0.46197916, 0.51902952, 0.47729224, 0.49780448, 0.43643696,
		 0.43925694, 0.44389609, 0.35277285, 0.28609592, 0.26872601,
		 0.27324457, 0.25      , 0.25      , 0.28596199, 0.28496165,
		 0.27944521, 0.31487537, 0.31783105, 0.26147158, 0.26260083,
		 0.28003055, 0.25      , 0.2656071 , 0.25      , 0.25      ,
		 0.29314237, 0.25      , 0.30179301, 0.26469974, 0.31683071,
		 0.31683071, 0.36411416, 0.30066045, 0.35496088, 0.38726906,
		 0.38718735, 0.34872038, 0.39953813, 0.38909558, 0.36384868,
		 0.38438698, 0.41308293, 0.35286198, 0.40407451, 0.36689594,
		 0.36852854, 0.38303674, 0.35169046, 0.41676364, 0.49438664,
		 0.50435967, 0.52277918, 0.57129906, 0.52389135, 0.46742603,
		 0.44868414, 0.39468027, 0.36778718, 0.39615823, 0.37711048,
		 0.41065619, 0.45909387, 0.44309215, 0.45259352, 0.48311083,
		 0.48326396, 0.49934983, 0.53214227, 0.48185315, 0.50021106,
		 0.50021106, 0.49792991, 0.49696264, 0.5138582 , 0.48502124,
		 0.48263586, 0.41147778, 0.48829066, 0.53143059, 0.53059244,
		 0.54700705, 0.54126735, 0.5276254 , 0.50401921, 0.50131194,
		 0.53145459, 0.52461903, 0.48181949, 0.50367399, 0.51124169,
		 0.52958012, 0.49554405, 0.45194492, 0.52860801, 0.49888832,
		 0.49383816, 0.4425334 , 0.3931658 , 0.35039396, 0.36349657,
		 0.38019912, 0.39449401, 0.31914212, 0.28131506, 0.266022  ,
		 0.30971744, 0.30418293, 0.32342109, 0.34592153, 0.3143378 ]]])
rf2=np.array([[ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.]])
# rf2=np.flip(rf2,1)

rf2=rf2/3.0
# rf1=np.flip(rf1,1)
rf1=np.array([[ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 3.,  0.],
	   [ 3.,  0.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.],
	   [ 0., -3.]])
rf1=rf1/3.0


reward_function_timeseries=np.concatenate((rf2,rf1),axis=0)

#CREATE SYNTHETIC DATA

all_data3,outcomes3,rewards3,choices3,safes3,preds3=simulate_one_step_featurelearner_counterfactual_2states_2iRF_sticky(num_subjects,num_trials,lrf2,lrv,betas,mf_betas,bandits,reward_function_timeseries,pred_change,rew_change,lr_cfr,lr_cfp,rfi_rews,rfi_preds,stick)
all_sim_data.append(all_data3)
#run logistic regression models on pilot data


import random
import pandas as pd
from scipy import optimize
import numpy as np
import os
from scipy.special import logsumexp
from numpy.random import beta,gamma,chisquare,poisson,uniform,logistic,multinomial,binomial
from numpy.random import normal as norm
import pandas as pd
import itertools
import multiprocessing
import seaborn  as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from scipy.optimize import minimize as mins
import concurrent.futures
import time
from itertools import repeat
from multiprocessing import Pool




def feature_learner_lik_beta_counterfactual_two_empirical_2lr_rfinsensitive_sticky(samples,data,rng_samples):
	from scipy.special import logsumexp
	sample_size=len(rng_samples)

	num_choices=2
	q_values=np.zeros((sample_size,num_choices))
	last_choice=np.zeros((sample_size,num_choices))
	safe_q=np.zeros((sample_size,num_choices))
	pred_q=np.zeros((sample_size,num_choices))
	lik=np.zeros((sample_size,))
	lr_cf=samples[7][rng_samples]
	lr_cf=lr_cf.reshape(sample_size,1)
	pred_changer=samples[5][rng_samples]
	rew_changer=samples[6][rng_samples]
	lr_val=samples[4][rng_samples]
	lr_features=samples[1][rng_samples]
	lr_features=lr_features.reshape(sample_size,1)
	betas_safe=samples[2][rng_samples]
	betas_pred=samples[3][rng_samples]
	rfi_rew=samples[8][rng_samples]
	rfi_pred=samples[9][rng_samples]
	beta_last=samples[10][rng_samples]
	beta_last=beta_last.reshape(sample_size,1)

	mf_betas=samples[0][rng_samples]
	mf_betas=mf_betas.reshape(sample_size,1)

	data=data.reset_index(drop=True)

		
	#grab relevant data
	choice=data.chosen_state

	# for i in data.version:
	#     version=i
	# if version.endswith('b')==True:
	#     reverse='yes'
	# else:

	reward_function_timeseries=data.rf
	outcome=data.reward_received
	safes=data.safes
	predators=data.preds
  
	for index in range(len(choice)):

		if index>=80:
			all_betas=np.concatenate((betas_safe.reshape(sample_size,1)-rew_changer.reshape(sample_size,1),betas_pred.reshape(sample_size,1)-pred_changer.reshape(sample_size,1)),axis=1)
		else:
			all_betas=np.concatenate((betas_safe.reshape(sample_size,1)+rew_changer.reshape(sample_size,1),betas_pred.reshape(sample_size,1)+pred_changer.reshape(sample_size,1)),axis=1)

	 
		current_reward_function=reward_function_timeseries[index]
		current_weights=all_betas*current_reward_function
		feature_matrix1=np.concatenate((safe_q[:,0].reshape(sample_size,1),pred_q[:,0].reshape(sample_size,1)),axis=1)
		q_sr1=np.sum((feature_matrix1*current_weights),axis=1) 
		feature_matrix2=np.concatenate((safe_q[:,1].reshape(sample_size,1),pred_q[:,1].reshape(sample_size,1)),axis=1)
		q_sr2=np.sum((feature_matrix2*current_weights),axis=1)
		q_sr=np.concatenate((q_sr1.reshape(sample_size,1),q_sr2.reshape(sample_size,1)),axis=1)

		# rf_insensitive_weights=betas_rfinsensitive_r*np.array((1.0,0))
		# q_rfi1=np.sum((feature_matrix1*rf_insensitive_weights),axis=1) 
		# q_rfi2=np.sum((feature_matrix2*rf_insensitive_weights),axis=1)
		# q_rfi_rew=np.concatenate((q_rfi1.reshape(sample_size,1),q_rfi2.reshape(sample_size,1)),axis=1)

		rf_insensitive_weights=np.concatenate((rfi_rew.reshape(sample_size,1),rfi_pred.reshape(sample_size,1)*-1),axis=1)
		q_rfi1=np.sum((feature_matrix1*rf_insensitive_weights),axis=1) 
		q_rfi2=np.sum((feature_matrix2*rf_insensitive_weights),axis=1)
		q_rfi_pred=np.concatenate((q_rfi1.reshape(sample_size,1),q_rfi2.reshape(sample_size,1)),axis=1)
		# +q_rfi_rew add in if want reward insensitive

		q_integrated=q_sr+(q_values*mf_betas)+q_rfi_pred+(last_choice*beta_last)
		lik = lik + q_integrated[:,int(choice[index])-1]-logsumexp(q_integrated,axis=1)
		pe = outcome[index]-q_values[:,int(choice[index])-1]
		pe_safe=safes[index]-safe_q
		pe_pred=predators[index]-pred_q
		
		current_decision=int(choice[index])-1
		last_choice=np.zeros((sample_size,num_choices))
		last_choice[:,int(choice[index])-1]=1

		other_decision=abs((int(choice[index])-1)-1)
		safe_q[:,current_decision]=((safe_q[:,current_decision].reshape(sample_size,1)+(lr_features*pe_safe[:,current_decision].reshape(sample_size,1)))).flatten()
		safe_q[:,other_decision]=((safe_q[:,other_decision].reshape(sample_size,1)+(lr_cf*pe_safe[:,other_decision].reshape(sample_size,1)))).flatten()
		pred_q[:,current_decision]=((pred_q[:,current_decision].reshape(sample_size,1)+(lr_features*pe_pred[:,current_decision].reshape(sample_size,1)))).flatten()
		pred_q[:,other_decision]=((pred_q[:,other_decision].reshape(sample_size,1)+(lr_cf*pe_pred[:,other_decision].reshape(sample_size,1)))).flatten()
		
		q_values[:,int(choice[index])-1]=q_values[:,int(choice[index])-1]+ (lr_val*pe)

	return lik
# In[10]:
import pandas as pd
from itertools import repeat,starmap
import numpy as np
import time
import scipy
from numpy.random import beta,gamma,chisquare,normal,poisson,uniform,logistic,multinomial,binomial

import pandas as pd
from itertools import repeat,starmap
import numpy as np
import time
import scipy
from numpy.random import beta,gamma,chisquare,normal,poisson,uniform,logistic,multinomial,binomial




#function to sample parameters within a model

def sample_parameters(distribution_type,hyperparameter_list,sample_size):
	from numpy.random import beta,gamma,chisquare,poisson,uniform,logistic,multinomial,binomial
	from numpy.random import normal as norm
	counter=1
	for num in hyperparameter_list:
		exec("global hp_{}; hp_{}={}".format(counter,counter,num))
		counter+=1
	exec("global sample; sample={}({},{},{})".format(distribution_type,hp_1,hp_2,sample_size))

	return sample
	
# MODEL CLASS: assumes a hierarchical model 
# population parameters and subject level parameters 
# are jointly fit

#  The structure of the model class:
#         GROUP LEVEL:
#              Name - E.g., Standard Q Learning
#              Sample Size - How many samples for each parameter
#              Lik - Likelihood function
#              iBIC - Total evidence accounting for model complexity
#              Total_Evidence: Sum of Subject-Level Evidences (sum b/c in log-space)
#              Parameters (entries below x #parameters):
#                     Hyperparameters - e.g., [mu,sigma]
#                     Distribution Type - e.g., Beta
#                     Name - e.g., Lrate-Value
#
#        SUBJECT LEVEL: 
#           Evidence (i.e., log mean likelihood)
#           Parameters (entries below x #parameters):       
#                 posterior mean
#                 credible interval (95%)
#                 samples
#                 num_good_samples (not non or inf samples)




class model:
	
	def __init__(self):
		self.name=0
		self.num_subjects=0
		self.params=self.groupParams()
		self.subjfit=self.subjFit()
		self.subj_level_info=self.subjFit.subjParams()
		self.sample_size=0
		self.model_evidence_group=0
		self.bic=10**10 #arbitrarily high starting iBIC
		self.lik_func=0 #likelihood function
		
	
	class groupParams:
		
		def __init__(self):
			self.name='eta'
			self.distribution='beta'
			self.hyperparameters=[1,2]
							
	class subjFit:
		def __init__(self):
			self.model_evidence_subject=0
			self.subj_level_info=self.subjParams()
			
		class subjParams:
			def __init__(self):
				self.posterior_mean=0
				self.credible_interval=0
				self.samples=[]
				self.num_good_samples=[] #not nan or inf
				

#retrieve list of parameters from model
def get_parameters_for_model(model):
	parameters=[]
	parameter_dict={}
	for var in vars(model):
		exec('global x; x={}.{}'.format(model.name,var))
		if type(x)==model.groupParams:
			if var!='params':
				parameters.append(var)

	return parameters

def get_parameters_for_model_parallel(model):
	parameters=[]

	for var in vars(model):
		exec('global x; x={}.{}'.format(model.name,var))
		if type(x)==model.groupParams:
			if var!='params':
				param_info=[]
				param_info.append(var)
				exec('param_info.append({}.{}.distribution)'.format(model.name,var))
				exec('param_info.append({}.{}.hyperparameters)'.format(model.name,var))
				parameters.append(param_info)

	return parameters

def build_model(name,likelihood,group_parameters_info,number_subjects,sample_size):
	from scipy.stats import beta,gamma,norm,poisson,uniform,logistic
	#  INPUTS:
	#     name = name of model
	#     likelihood = likelihood function
	#     group_parameter_info = *Python dictionary* 
	#       defining parameter names, distributions and hyperparameters
	#       EXAMPLE: {'eta':['beta',[1,1]]}
	#     sample_size = number of samples from prior over group parameters
	
	#  OUTPUTS:
	#     model class (see above)
	
	exec('{}=model()'.format(name),globals())
	exec('{}.name="{}"'.format(name,name))
	exec('{}.num_subjects={}'.format(name,number_subjects))
	exec('{}.lik_func={}'.format(name,likelihood))
	exec('{}.sample_size={}'.format(name,sample_size))
	
	#encode in model the number of subjects and parameters in one's data
	for parameter in group_parameters_info:
		exec('{}.{}={}.groupParams()'.format(name,parameter,name))
		exec('{}.{}.name="{}"'.format(name,parameter,parameter))
		exec('{}.{}.distribution="{}"'.format(name,parameter,group_parameters_info[parameter][0]))
		exec('{}.{}.hyperparameters={}'.format(name,parameter,group_parameters_info[parameter][1]))
		exec('{}.{}.sample_size={}'.format(name,parameter,sample_size))
		exec('{}.{}.samples=sample_parameters("{}",{},{})'.format(name,parameter,group_parameters_info[parameter][0],group_parameters_info[parameter][1],sample_size))

	for sub in range(number_subjects):
		exec('{}.subject{}={}.subjFit()'.format(name,sub,name))
		for parameter in group_parameters_info:
			exec('{}.subject{}.{}={}.subject{}.subjParams()'.format(name,sub,parameter,name,sub))

def each_param(parameter,model,valid_parameter_indices,weights,subject,good_samples):
	exec('global parameter_samples; parameter_samples={}.{}.samples'.format(model.name,parameter))
	parameter_samps=np.reshape(parameter_samples,(model.sample_size,1))
	good_parameters=parameter_samps[valid_parameter_indices]

	mean,ci,sample=compute_samples(good_parameters,weights)
	exec('{}.subject{}.{}.posterior_mean={}'.format(model.name,subject,parameter,mean))
	exec('{}.subject{}.{}.credible_interval={}'.format(model.name,subject,parameter,ci))
	exec('{}.subject{}.{}.samples={}'.format(model.name,subject,parameter,sample))
	exec('{}.subject{}.{}.num_good_samples={}'.format(model.name,subject,parameter,good_samples))

from numpy.random import choice
import random
def each_param_parallel(parameter_samples,valid_parameter_indices,weights):
	parameter_samps=np.array(parameter_samples)
	good_parameters=parameter_samps[valid_parameter_indices]
	df=pd.DataFrame()
	weights=np.divide(weights,np.sum(weights))
	df['weights']=weights
	df['parameter_samples']=parameter_samps    
	df2=df.sort_values('parameter_samples')
	df2=df2.reset_index(drop=True)
	mean=np.sum(df['weights']*df['parameter_samples'])
	cdf=df2.weights.cumsum()
	cdf=np.array(cdf)
	samples_unweighted=list(df2['parameter_samples'])
	likelihood_weights=list(df2['weights'])
	samples = random.choices(samples_unweighted,cum_weights=list(cdf),k=10000)
	samples=list(samples)
	index_5=next(x[0] for x in enumerate(cdf) if x[1] > 0.0499999)
	index_95=next(x[0] for x in enumerate(cdf) if x[1] > 0.9499999) 
	ci_lower=df2['parameter_samples'][index_5]
	ci_higher=df2['parameter_samples'][index_95]
	ci=[ci_lower,ci_higher]
	results=[mean,ci,samples]
	return results


##from numba import vectorize, float64


def derive_posterior_samples(likelihood_vector,subject):    
	not_infs= ~np.isinf(likelihood_vector)
	not_nans= ~np.isnan(likelihood_vector)
	valid_parameter_indices=not_infs==not_nans     

	likelihood_vector_noinf=likelihood_vector[~np.isinf(likelihood_vector)] 
	likelihood_vector_cleaned=likelihood_vector_noinf[~np.isnan(likelihood_vector_noinf)] 
	good_samples=len(likelihood_vector_cleaned) 

	weights=np.exp(likelihood_vector_cleaned) 
	
	return likelihood_vector_cleaned,valid_parameter_indices,weights,good_samples
	
		


def compute_samples(parameter_samples,weights):
	import time
	import pandas as pd
	
	indices=np.array(indices)
	samples=df2['parameter_samples'][indices]
	samples=list(samples)
	index_5=next(x[0] for x in enumerate(cdf) if x[1] > 0.0499999) 
	index_95=next(x[0] for x in enumerate(cdf) if x[1] > 0.9499999) 
	ci_lower=df2['parameter_samples'][index_5]
	ci_higher=df2['parameter_samples'][index_95]
	ci=[ci_lower,ci_higher]
	
	return val,ci,samples


#fit hyperparameters to group sampled values from posterior. Be sure that you make sure output
# is appropriately handled for non-traditional (i.e., not beta, gamma, normal) distribution


def fit_hyperparameters(model):
	from scipy.stats import beta,gamma,norm,poisson,uniform,logistic
	parameters=get_parameters_for_model(model)
	number_subjects=model.num_subjects
	model_name=model.name
	for parameter in parameters:
		parameter_full_sample=[]
		exec('global distribution; distribution={}.{}.distribution'.format(model_name,parameter))
		for subject in range(number_subjects):
			exec('parameter_full_sample+={}.subject{}.{}.samples'.format(model_name,subject,parameter))
		exec('global hyperparameters; hyperparameters={}.fit(parameter_full_sample)'.format(distribution))
		if distribution=='gamma':
			h1=hyperparameters[0]
			h2=hyperparameters[2]
		elif distribution=='uniform':
			h1=hyperparameters[0]
			h2=hyperparameters[1]+hyperparameters[0]
		else:
			h1=hyperparameters[0]
			h2=hyperparameters[1]
		exec('{}.{}.hyperparameters={}'.format(model.name,parameter,[h1,h2]))



def fit_hyperparameters_parallel(model,parameter_info,all_results,number_subjects):
	from scipy.stats import beta,gamma,norm,poisson,uniform,logistic
	for parameter in parameter_info:
		parameter_full_sample=[]
		for item in all_results:
			for items in item:
				if parameter[0] in items:
					parameter_full_sample+=items[3]

		
		if parameter[1]=='gamma':
			hyperparameters=gamma.fit(parameter_full_sample,floc=0)
			h1=hyperparameters[0]
			h2=hyperparameters[2]
		elif parameter[1]=='uniform':
			hyperparameters=uniform.fit(parameter_full_sample)
			h1=hyperparameters[0]
			h2=hyperparameters[1]+hyperparameters[0]
		elif parameter[1]=='norm':
			hyperparameters=norm.fit(parameter_full_sample)
			h1=hyperparameters[0]
			h2=hyperparameters[1]
		else:
			parameter_full_sample=[0.0001 if i<0 else i for i in parameter_full_sample]
			parameter_full_sample=[0.9999 if i>1 else i for i in parameter_full_sample]
			hyperparameters=beta.fit(parameter_full_sample,floc=0,fscale=1)
			h1=hyperparameters[0]
			h2=hyperparameters[1]
		exec('{}.{}.hyperparameters={}'.format(model.name,parameter[0],[h1,h2]))


def sample_group_distributions(model):

	parameters=get_parameters_for_model(model)
	number_subjects=model.num_subjects
	model_name=model.name
	for parameter in parameters:
		exec('global distribution; distribution={}.{}.distribution'.format(model_name,parameter))
		exec('global hyperparameters; hyperparameters={}.{}.hyperparameters'.format(model_name,parameter))
		exec('{}.{}.samples=sample_parameters("{}",{},{})'.format(model_name,parameter,distribution,hyperparameters,model.sample_size))


def sample_group_distributions_parallel(params,sample_size):
	all_parameter_samples=[]
	for parameter in params:
		param=parameter[0]
		distribution=parameter[1]
		hyperparameters=parameter[2]
		samples=sample_parameters(distribution,hyperparameters,sample_size)
		all_parameter_samples.append(samples)
	return all_parameter_samples



def get_total_evidence(model):
	number_subjects=model.num_subjects
	model_name=model.name
	group_model_evidence=0
	for subject in range(number_subjects):
		exec('global subjEvidence; subjEvidence={}.subject{}.model_evidence_subject'.format(model_name,subject))
		group_model_evidence+=subjEvidence
	return group_model_evidence



def process_subject(subject,parameter_info,all_data,lik_func,parameter_sample_size,samples_partitioned,cores):
		data=all_data[subject]
		data=data.reset_index()
		parameter_names=[x[0] for x in parameter_info]
		samples_a=sample_group_distributions_parallel(parameter_info,parameter_sample_size)

		inputs=zip(repeat(samples_a),repeat(data),samples_partitioned)
		rng=np.arange(0,parameter_sample_size,1)
		
		likelihood=lik_func(samples_a,data,rng)
		likelihood=np.array(likelihood)

		#parallelize likelihood
		# if __name__=='__main__':
		#     pool = Pool(processes=cores)
		#     results=pool.starmap(lik_func, inputs)
		#     pool.close()
		#     pool.join()
		#     likelihood = [item for sublist in results for item in sublist]


		likelihood_vector_cleaned,valid_parameter_indices,weights,good_samples=derive_posterior_samples(likelihood,subject)
		# log mean likelihood
		model_evidence=scipy.special.logsumexp(likelihood_vector_cleaned,axis=0)-np.log(good_samples)
		#save  data
		subject_name='subject_{}'.format(subject)
		#return_dict_info=[[data.subjectID[10]]]
		return_dict_info=[[subject]]
		return_dict_info.append([model_evidence])
		#resample parameters and finish saving data
		# counter=0
		# inputs=zip(samples_a,repeat(valid_parameter_indices),repeat(weights))
		# if __name__=='__main__':
		#     pool = Pool(processes=len(parameter_names))
		#     results=pool.starmap(each_param_parallel, inputs)
		#     pool.close()
		#     pool.join()
		counter=0
		#resampling
		for param in parameter_names:
			new_samples=each_param_parallel(samples_a[counter],valid_parameter_indices,weights)
			ep=[]
			ep.append(param)
			ep.append(new_samples[0])
			ep.append(new_samples[1])
			ep.append(new_samples[2])
			# ep.append(results[counter][0])
			# ep.append(results[counter][1])
			# ep.append(results[counter][2])
			return_dict_info.append(ep)
			counter+=1

		return return_dict_info
# # Build models
# In[11]:



#define the number of subjects, parameters, and hyperparameters (i.e., parameters for group prior)

# # Complex Feature Learner -- Different learning rates for each feature
# name_1='feature_learner'
# number_subjects=num_subjects
# parameter_sample_size=96000
#                             #name of parameters and hyperpriors
# group_parameters_info_mood={'inv_temp':['gamma',[1,1]],'lr_features_safes':['beta',[1,1]],
#                             'lr_features_milds':['beta',[1,1]],'lr_features_preds':['beta',[1,1]],
#                             'weight':['beta',[1,1]],'lr_val':['beta',[1,1]]} 
# likelihood='feature_learner_lik'
# build_model(name_1,likelihood,group_parameters_info_mood,number_subjects,parameter_sample_size)
# all_models.append(feature_learner)


# Complex Feature Learner -- Different learning rates for each feature
	#(mf_betas,lr_features,beta-1-2-3,lrv,)







import pickle
#### NUMBER SUBJECT #####
for iteration in range(1):

	all_models=[]
	number_subjects=192
	#number_subjects=40
	# stop

	name_1='winning_model'
	parameter_sample_size=100000

								#name of parameters and hyperpriors
	group_parameters_info_mood={
								'be_mf':['gamma',[1,1]],
								'lr_neg':['beta',[1,1]],
								'safe_b':['gamma',[1,1]],
								'pred_b':['gamma',[1,1]],
								'lr_val':['beta',[1,1]],
								'pre_change':['norm',[0,1]],
								're_change':['norm',[0,1]],
								'lr_cf':['beta',[1,1]],
								'rfi_rew':['norm',[0,1]],
								'rfi_pred':['norm',[0,1]],
								'stick':['norm',[0,1]]
								}
								# 'lr_cfac_p':['beta',[1,1]]} 
	likelihood='feature_learner_lik_beta_counterfactual_two_empirical_2lr_rfinsensitive_sticky'
	build_model(name_1,likelihood,group_parameters_info_mood,number_subjects,parameter_sample_size)
	all_models.append(winning_model)


	#Partition data for parallelization


	x=np.arange(number_subjects)
	cores_subs=12
	bins=np.arange(0,number_subjects-1,number_subjects/cores_subs)
	subjects_partitioned=[]
	for i in range(1,cores_subs+1):
		subjects_partitioned.append(x[np.digitize(x,bins)==i])

	#divide up likelihood over all cores
	x=np.arange(parameter_sample_size)
	cores=4
	bins=np.arange(0,parameter_sample_size-1,parameter_sample_size/cores)
	samples_partitioned=[]
	for i in range(1,cores+1):
		samples_partitioned.append(x[np.digitize(x,bins)==i])


	
	import concurrent.futures
	from multiprocessing.dummy import Pool
	import multiprocessing
	from itertools import repeat,starmap
	import numpy as np
	import time


	#current_dataset=all_data_input #make sure dataset is correct before running model fitting!!
	current_dataset=all_sim_data[iteration]

	#iterate over models
	for models in all_models:
		

		improvement=1000000 #arbitrary start to ensure while loop starts
		
		#keep sampling until no improvement in iBIC
		while improvement>0:
			
			#store old_bic for comparison to new_bic
			old_bic=models.bic

			
			#generate log likelihood for each subject and compute new samples
			procs = []
			parameter_info=get_parameters_for_model_parallel(models)
			
			
			print(parameter_info)
			parameter_names=[x[0] for x in parameter_info]
			parameter_disributions=[x[1] for x in parameter_info]
			parameter_sample_size=models.sample_size
			subjects=list(np.arange(0,number_subjects,1))
		   
			lik_func=models.lik_func
			return_dict={}
			inputs=zip(subjects,repeat(parameter_info),repeat(current_dataset),
				repeat(lik_func),repeat(parameter_sample_size),repeat(samples_partitioned),repeat(cores))
			# start_time = time.time()
			if __name__=='__main__':    
					pool = Pool(processes=cores_subs)
					results=pool.starmap(process_subject, inputs)
					pool.close()
					pool.join()
					
					# print('total time: {}'.format(time.time()-start_time))
					exec('all_results_{} = [item for item in results]'.format(models.name))
					
			# print('total time: {}'.format(time.time()-start_time))

			exec('all_results=all_results_{}'.format(models.name))
			#fit new hyperparameters from full posterior
			fit_hyperparameters_parallel(models,parameter_info,all_results,num_subjects)
			
			#Compute iBIC
			Nparams = 2*len(parameter_names)
			Ndatapoints = float(number_subjects*num_trials) #total number of datapoints
			exec('total_evidence=sum([x[1][0] for x in all_results_{}])'.format(models.name))
			new_bic = -2.0*float(total_evidence) + Nparams*np.log(Ndatapoints) # Bayesian Information Criterion
			improvement = old_bic - new_bic # compute improvement of fit
			
			#only retain evidence and BIC if they improve
			if improvement > 0:
				models.model_evidence_group=total_evidence
				models.bic=new_bic
			
			#read out latest iteration
			print('{}- iBIC old:{}, new: {}\n'.format(models.name, old_bic, new_bic))
			


	pred_betas=[]
	safe_betas=[]
	for subject in range(num_subjects):
		pred_betas.append(all_results[subject][5][1])
		safe_betas.append(all_results[subject][4][1])

	median_difference = np.median(safe_betas)-np.median(pred_betas)
	dscores=[safe_betas[i]-pred_betas[i] for i in range(len(pred_betas))]
	mean_difference=np.mean(safe_betas)-np.mean(pred_betas)

	std_difference= np.sqrt(np.std(safe_betas)/number_subjects+np.std(pred_betas)/number_subjects)
	t_score=mean_difference/std_difference
	mean_difference_log = np.mean(safe_betas)-np.mean(pred_betas)
	# print('mean difference log this iteration: {}'.format(mean_difference_log))
	mean_differences.append(median_difference)
	mean_differences.append(mean_difference_log)

	mean_differences.append(t_score)



	gcs=storage.Client()
	data=[['median_diff','mean_diff','t_difference',]]
	data.append(mean_differences)
	df = pd.DataFrame(data[1:],columns=data[0])
	df.to_csv('local_file.csv')
	gcs.bucket('mfitmb-output').blob('iterCSV_MBpermit_{}.csv'.format(np.random.randint(10000000))).upload_from_filename('local_file.csv', content_type='text/csv')


