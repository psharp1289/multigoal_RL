{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MB + RR+ MF + H  model\n",
    "\n",
    "i=state, j=feature, t=trial\n",
    "\n",
    "### Model-based control\n",
    "\n",
    "Update chosen state:\n",
    "\n",
    "$\\text{feature}_{i,j,t+1}=\\text{feature}_{i,j,t} + \\alpha_{chosen}(\\text{O}_{feature,t}-\\text{feature}_{i,j,t})$\n",
    "\n",
    "Update counterfactual feedback:\n",
    "\n",
    "$\\text{feature}_{i,j,t+1}=\\text{feature}_{i,j,t} + \\alpha_{countefactual}(\\text{O}_{feature,t}-\\text{feature}_{i,j,t})$\n",
    "\n",
    "$\\vec{Attention} = \\begin{bmatrix}\n",
    "\\beta_{reward}\\\\\n",
    "\\beta_{predator}\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\vec{\\text{Instructed Reward Function}} =\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}$ or\n",
    "$\\begin{bmatrix}\n",
    "0\\\\\n",
    "-1\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Reward RF Trial: \n",
    "$QMB =\\begin{bmatrix}\n",
    "\\beta_{reward}\\text{feature}_{a1,rew} & \\beta_{predator}\\text{feature}_{a1,pred}  \\\\\n",
    "\\beta_{reward}\\text{feature}_{a2,rew} & \\beta_{predator}\\text{feature}_{a2,pred}  \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### Resource rational attentional shifting\n",
    "\n",
    "In reward-rich environment (80 trials, counterbalanced):\n",
    "\n",
    "$\\vec{Attention} = \\begin{bmatrix}\n",
    "\\beta_{reward} + \\beta_{change,reward}\\\\\n",
    "\\beta_{predator}+ \\beta_{change,predator}\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "In predator-rich environment (80 trials, counterbalanced):\n",
    "\n",
    "$\\vec{Attention} = \\begin{bmatrix}\n",
    "\\beta_{reward} - \\beta_{change,reward}\\\\\n",
    "\\beta_{predator} - \\beta_{change,predator}\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "### MF = Model-free learning\n",
    "\n",
    "$\\text{QMF}_{i,j,t+1}=\\text{QMF}_{i,j,t} + \\alpha_{MF}(\\text{O}_{reward,t}-\\text{QMF}_{i,j,t})$\n",
    "\n",
    "### H = Heuristic Strategy\n",
    "\n",
    "$QH=\\begin{bmatrix}\n",
    "\\beta_{HeuristicReward}\\text{feature}_{a1,rew} & \\beta_{HeuristicPredator}\\text{feature}_{a1,pred}  \\\\\n",
    "\\beta_{HeuristicReward}\\text{feature}_{a2,rew} & \\beta_{HeuristicPredator}\\text{feature}_{a2,pred}  \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1\\\\\n",
    "-1\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### Choice Perseveration\n",
    "$\\vec{\\text{LastAction}} =\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\beta_{perseveration} \\sim \\mathcal{N}(\\mu,\\sigma)$\n",
    "\n",
    "### Stochastic choice process\n",
    "\n",
    "$\\forall{i}, Q_i = \\beta_{MF}QMF_i + QMB_i + QH_i+\\beta_{perseveration}\\text{LastAction}$\n",
    "\n",
    "$p(Choice=1) = \\frac{e^{Q_i}}{\\sum_{k}{e^{Q_k}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
